
In this chapter, we discuss important related work that will influence, directly and indirectly, later contributions. \note{Can you say briefly what dimensions of relatedness you are using?  How are you deciding what to include and what not to include?}



\section{Bayes-adaptive Markov decision processes}

The Bayes-adaptive MDP, or BAMDP~\cite{duff03}, is an MDP whose optimal policy is the Bayes-optimal policy for some other unknown MDP that is drawn from a known prior. Every state in the BAMDP is a belief-state composed of a history, or the set of observations made by the agent so far, and the concrete state that the agent is currently in.

The transition function of the BAMDP is defined as follows:
\begin{eqnarray}
\label{rel:bamdp:prob}
P(\langle s', h + (s,a,s',r)\rangle, r | \langle s, h\rangle, a) =& \int_M P(s', r | s, a, M)\phi(M|h) dM,
\end{eqnarray}
\note{pp: Why is the distribution over h?  Should it be conditioned on h only?}
where $\langle s, h\rangle$ is the belief-state where the agent begins, and is composed of the concrete state $s$ and history $h$. The value $h$ is the summary of all transitions observed by the agent over the course of its lifetime. Here, they are represented by $(s, a, s',r)$ tuples detailing the beginning and ending concrete\note{undefined.  should introduce this term earlier.} states, the action taken and the reward received.

The likelihood \note{?} is taken by integrating out the underlying ``true'' MDP, according to MDP's posterior distribution. The generative process \note{?} is equivalent to first sampling an MDP $M$ from the posterior distribution $\phi(M|h)$, and then sampling the next state and reward $s', r$ from the MDP.

By constructing the BAMDP according to Equation~\ref{rel:bamdp:prob}, the optimal policy is identical to the Bayes-optimal policy in the learning setting. This fact follows because the succession of transition likelihoods is exactly the same as those indicated in the Bayesian model. \note{Can you sum up why that's important?}

Unfortunately, the BAMDP has potentially an infinite number of states, since each possible history maps to a different belief state. However, as the number of observations from a particular concrete state and using a particular action grows large, the distribution over the possible next concrete states converges, allowing a clever algorithm designer to collect large sets of belief states together into equivalent classes, for the purposes of planning. \note{Has that been done?  What is the status of this algorithm?}



\section{Bayesian Dynamic Programming}

\begin{algorithm}[tb]
	\caption{$\mbox{Bayesian~DP}(s, \Phi, K)$}
	\label{alg:bdp}
	\KwIn{initial state $s$, prior $\Phi$, \#steps $K$}
	$O \leftarrow \{\}$\\
	\For {ever} {
		$M \sim \Phi|O$ \bf{ZZZ} pp: Be more precise by making it clear that this is a distribution over phi given O.\\
		$\pi \leftarrow \mbox{solve}(M)$\\
		\For {$K$ times} {
			$a \leftarrow \pi(s)$\\
			$s', r \leftarrow \mbox{perform}(a)$\\
			$O \leftarrow O \cup \{(s, a, s', r)\} $\\
			$s \leftarrow s'$
		}
	}
\end{algorithm}

Bayesian Dynamic Programming~\cite{strens00}, or \alg{Bayesian~DP}, is one of the first model-based reinforcement learning algorithms to take advantage of posterior sampling. The agent requires a model prior $\phi$ and a parameter $K$, which tells the agent how many steps should be taken between samples.

Algorithm~\ref{alg:bdp} lays out the \alg{Bayesian~DP} algorithm. The agent starts in some initial state with no information about the environment except that which can be gleaned from the prior $\Phi$. Before taking any actions, the agent samples a model $M$ from the posterior $\Phi|O$, where $\Phi$ is the model prior and $O$ is the set of observations recorded so far. Once $M$ is sampled from the posterior, it is solved optimally by whatever planning algorithm the designer prefers. The resulting policy $\pi$ is acted upon for the next $K$ steps, and each step is recorded and added to the set of observations. After $K$ actions have been performed, a new model is sampled from the updated posterior, and the process is repeated until the end of the experiment.

The act of periodically sampling from the posterior is intended to give the agent glimpses of possible worlds. Some of the times a sample is drawn, a state that the agent has not visited much will appear better than it actually is, drawing the agent towards this relatively unknown state. On the other hand, some times that unknown state will appear to have a very low value, causing the agent to avoid it. State-action pairs that the agent has experienced many times in the past will have relatively stable dynamics, and will usually have a consistent value with respect to its neighboring states. As a result, this approach strikes a balance between exploration of states in which it is uncertain and exploitation of states in which it is confident.

The fact that some samples will make an unknown state appear attractive and other samples will make the same unknown state appear unattractive requires \alg{Bayesian~DP} to be careful about when and how often it samples new models. Too long between samples, and the agent is acting based on stale information.  Too short between samples and the agent can end up dithering, as explained next.

The parameter $K$, which is the number of steps taken between samples, should be chosen to approximate the length of a trial (in situations where there is a goal state) or the width of the MDP (maximum expected number of steps to get from any given state to any other given state) when there is an infinite horizon.

The idea here is that the agent should be able to reach any particular state it wants before the next sample is taken and the agent might change its mind about where it is trying to go. Consider a simple \env{chain} MDP, where only the two states on the end are unknown. In a given posterior sample, one of the two ends may have a higher apparent value than the other. In the next sample, the relative values of these two states may switch. If a new sample is taken before the agent can reach one of the ends (and therefore learn about its dynamics), thrashing \note{pp: What does trashing mean here?  Why will it result in an exponential number of steps?} can occur, resulting in a very large number of steps (relative to the number of states) before knowledge is gained.

Spacing out the MDP samples temporally allows the agent to visit at least one unknown (and optimistically sampled) state every $K$ steps (assuming that any of the unknown states are optimistic in the posterior sample for those $K$ steps). \note{pp: Why is it important to space the MDP samples? Isn't it better to take into account the data collected as soon as it becomes available?}



\section{BEETLE}

Bayesian Exploration Exploitation Tradeoff in LEarning, or \alg{BEETLE}~\cite{poupart06} treats the task of Bayes-optimal behavior as optimal behavior in a POMDP whose hidden state is the MDP describing the transition probabilities between the observed states.

This algorithm assumes a flat Dirichlet multinomial prior, where each of the MDP's next-state distributions are drawn from a Dirichlet distribution. \alg{BEETLE} then uses an iterative method \note{pp: The iterative method is value iteration} to approximate the Bayes-expected value of each state, and makes decisions based upon these results. \note{pp: The value function is not over states only, but also beliefs, which correspond to distributions over the model parameters.}

\note{pp: We use Dirichlets to express distributions over the model parameters, but we don't draw any samples.  All integrals are computed exactly.}

\note{Given that Pascal is on the committee, it sure seems like this section should be extensive.  Currently, it's the shortest one.}


\section{Bayesian Sparse Sampling}

\alg{Bayesian Sparse Sampling}~\cite{wang05} is an adaptation of Sparse Sampling (see Section~\ref{sec:rel:ss}) that uses a model prior and a targetted method of choosing actions while planning.

Where Sparse Sampling \note{pp: Explain sparse sampling.} is only able to estimate a state's value for a particular concrete model, Bayesian Sparse Sampling works with uncertainty, codified by a provided model prior. Every time a next-state is needed to further build the search tree \note{pp:This is the first time that you talk about a search tree.  Explain the algorithm first and then talk about the resulting search tree.} , a model is sampled from the posterior, and the next-state is sampled from that model. Future next-states in the sub-tree of the newly sampled next-state will include this generated transition in their observation sets, allowing the sub-trees to simulate learning. \note{how is it not just sparse sampling?  in fact, the original sparse sampling paper (I'm pretty sure) observes that it applies equally as well to POMDPs as MDPs.  Isn't it, in a sense, just being applied to the BAMDP?}

The method of action-selection differs from Sparse Sampling in that rather than trying each action a number of times, Bayesian Sparse Sampling will run a series of roll-outs. note{define roll-out better} To choose the action at each stage in the roll-out, Bayesian Sparse Sampling samples a model from the posterior and finds the optimal action in the sampled model from the current state, and chooses that state, similar to Thompson sampling~\cite{thompson33}.

\note{wait, if it's doing roll outs, why does it need the sparse sampling tree at all?}

Choosing actions by identifying the optimal action in posterior samples allows Bayesian Sparse Sampling to target its exploration to attempt only actions that have a chance of being optimal. Also, when the posterior has converged to something close to the truth, the sampled model will be consistent and correct, and Bayesian Sparse Sampling will only choose the best action, wasting no samples in its value estimation.

This algorithm has some efficiency issues; it requires sampling from the posterior and solving for the true optimal action in the inner loop. Both posterior sampling and model solving can often be computationally difficult. There are cases where the posterior sampling and model solving can be done efficiently, for instance when the model is a bandit problem with the Beta prior~\cite{wang05}.



\section{Bayesian Exploration Bonus}

The \alg{BEB}~\cite{kolter09} algorithm is a computationally efficient method to achieve near Bayes-optimal behavior in discrete state- and action-spaces when using a Dirichlet prior.

The algorithm uses a flat Dirichlet multinomial (FDM) prior, which states that
\begin{eqnarray}
\theta^{s,a}&\sim&Dirichlet(\alpha),\\
P(s'|s,a)&=&\theta^{s,a}_{s'},
\end{eqnarray}
where $\alpha$ is a tuning parameter that correlates with variance in the next-state distribution.

\alg{BEB} assumes the reward function is known.

To drive exploration, \alg{BEB} adds an internal bonus $B(s,a)$ to the reward received when taking action $a$ from state $s$. It acts greedily according to $\hat Q(s,a)$:
\begin{eqnarray}
\hat Q(s,a)&=&R(s,a)+B(s,a)+\gamma \sum_{s'} P(s'|s,a)\hat V(s'),\\
\hat V(s)&=& \max_a \hat Q(s, a),\\
\label{eqn:rel:beb:bonus}
B(s,a)&=&\frac\beta{1+n(s,a)},
\end{eqnarray}
where $n(s,a)$ is the number of times that action $a$ has been taken from state $s$.

This algorithm is similar to Model-Based Interval Estimation (\alg{MBIE})~\cite{strehl06}, except that \alg{MBIE} uses the square root of the visit count in its bonus. This difference causes the bonus term used by \alg{MBIE} to decay more slowly resulting in slower convergence, and that extra conservativity allows \alg{MBIE} to be a PAC-MDP algorithm, where \alg{BEB} is a near Bayes-optimal algorithm when the prior is the flat Dirichlet multinomial.

\subsection{Variance-based BEB}

note{try to avoid having only one subsection.  reorganize?}

Variance-based {\bf BEB}, or {\bf VBEB}~\cite{sorg10}, is an adaptation of {\bf BEB} that can make use of a general prior, one that is not restricted to the flat Dirichlet multinomial.

The {\bf VBEB} algorithm provides a framework for analyzing an arbitrary prior, and creating a bonus term based on the posterior variance for the state-action pair to which the bonus will be applied. In this framework, the flat Dirichlet multinomial gets the bonus term from Equation~\ref{eqn:rel:beb:bonus}, and it will be different for other priors.

\note{can it handle all possible priors?  does it have guarantees?}



\section{RMAX}

The \alg{RMAX}~\cite{brafman03} algorithm is a simple and robust method to balance exploration and exploitation in the reinforcement-learning setting. The original algorithm addresses only discrete state- and action-spaces, but the ideas apply more generally~\cite{nouri09,jong07}.

To effectively use \alg{RMAX}, the agent designer needs a metric to determine whether or not a particular state-action pair is \emph{known}. In the discrete state and action case, the agent can count how many times it has tried a given action pair. If that number exceeds a predetermined threshold, the given pair is \emph{known}. Otherwise, it is \emph{unknown}.

To guide the \alg{RMAX} agent, a model is constructed from the agent's observations, with unknown state-action pairs transitioning only to a nirvana state, or a state with the highest possible value. Known states have the transition dynamics given by the MLE. The agent then acts greedily according to this model.

Clamping unknown states at very high value will draw the agent towards these states if there is any chance that visiting those states is the optimal thing to do.

Because the model maintained by \alg{RMAX} is accurate (when the state is known), optimistic (when the state is unknown) and there is a bounded number of times that the agent can be surprised (because there is a finite number of state-action pairs that can become known), the authors were able to show that the algorithm is PAC-MDP. That is, it will make only a small number of suboptimal decisions over the course of its lifetime.

\subsection{Potential-based RMAX}

\note{try to avoid lone subsections}

As it stands, the \alg{RMAX} algorithm cannot make use of any prior knowledge. All unknown states are assigned the highest possible value, and the agent is drawn to each of them in a way that depends only on how quickly it can reach them.

The \alg{Potential-based shaping RMAX}~\cite{asmuth08} algorithm addresses this deficiency by incorporating a potential function $\phi(s)$. The agent will perform well if this potential function is \emph{admissible}. That is, $\forall_s ~ V(s) \leq \phi(s)$; the potential function's value for a given state must never be less than the true value for the state. This admissibility requirement is common in heuristic-based search algorithms, such as A*~\cite{russell1995artificial}.

This algorithm works by assigning unknown states a value of $\phi(s)$, rather than $\Vmax$, the highest possible value. Setting $\phi(s)=\Vmax$ is allowed, since $\Vmax$ automatically satisfies the admissibility requirement, but lower values result in faster convergence.

On the downside, this algorithm is very conservative, like the rest of the \alg{RMAX} family. Requiring a fixed number of visits to a state-action pair before it becomes known means a lot of exploration steps, some of which might not be necessary in practice.

\note{not quite.  using V* for Phi leads to a perfect policy, right?  it is only as conservative as the shaping function makes it}



\section{Instance-based model learning}

Instance-based methods represent functions by a set of examples and use a predefined similarity function to generalize to novel inputs. In the reinforcement-learning setting, an instance is the tuple $\langle s, a, s', r \rangle$, or the combination of state, action, next state and reward.

Over the course of its lifetime, the agent gathers a large set of $\langle s, a\rangle \rightarrow \langle s', r \rangle$ mappings. When it observes a new state-action pair, it can compare it to those it has seen in the past and predict a next-state and reward that fits with those already observed.

The ideas behind instance-based model learning can be combined with the \alg{RMAX} algorithm to create {\bf Fitted-RMAX}~\cite{jong07}. {\bf Fitted-RMAX} uses a Gaussian kernel to determine the similarity between two states:
\begin{eqnarray}
d(s_1,s_2)&=&\exp\left(\frac{-||s_1 - s_2||^2_2}{b}\right),
\end{eqnarray}
where $b$ is a \emph{width} parameter used to determine how much distance affects the similarity between two states.

The possible next-states and rewards are then taken from a weighted average of those seen before when taking the action in question, where each possibility $\langle s'_i, r_i \rangle$ is weighted by $d(s_t, s_i)$, where $s_t$ is the state from which the agent is considering taking an action.

\note{one difference from how I remember this algorithm is that I think he used *relative* transitions, not absolute transitions.}

Since this algorithm is applied in continuous domains, the standard definition of knownness that uses visit counts cannot be used.  Instead, a distance metric needs to be incorporated. One way to find one is to take the sum of the similarity scores between the state in question and all previously experienced states, and declare a threshold $B$, over which the knownness metric declares a state to be known. \note{that's one way… is that what the algorithm does?} Unknown states are then assigned maximum value, according to the rules of \alg{RMAX}.

Once a policy is found for the {\bf Fitted-RMAX} model, using some existing continuous-state planner, the agent will be drawn to explore states that are either unknown or have high value, like the original \alg{RMAX} on which it is based.



\section{Sparse Sampling}

\label{sec:rel:ss}

\begin{algorithm}[tb]
	\caption{$\mbox{Sparse~Sampling}(s, P, R, \gamma, \epsilon, \delta)$}
	\label{alg:ss}
	\KwIn{initial state $s$, next-state sampler $P$, reward function $R$, discount factor $\gamma$, accuracy $\epsilon$, probability of failure $\delta$}
	\KwOut{estimated value $V$}
	$\Rmax \leftarrow \max_{sa}R_{sa}$\\
	$\Vmax \leftarrow \frac{\Rmax}{1-\gamma}$\\
	$\lambda \leftarrow \frac{\epsilon(1-\gamma)^2} 4$\\
	$k \leftarrow \#\mbox{actions}$\\
	$H \leftarrow \left\lceil\log_\gamma(\lambda/\Vmax)\right\rceil$\\
	$C \leftarrow \frac {\Vmax^2}{\lambda^2}\left(2H\log\frac{kH\Vmax^2}{\lambda^2}+\log\frac{\Rmax}{\lambda}\right)$\\
	$V \leftarrow \mbox{Sparse~Sampling~Recursion}(s, P, R, \gamma, H, C)$\\
	\Return $V$
\end{algorithm}

\begin{algorithm}[tb]
	\caption{$\mbox{Sparse~Sampling~Recursion}(s, P, R, \gamma, d, C)$}
	\label{alg:ssr}
	\KwIn{initial state $s$, next-state sampler $P$, reward function $R$, discount factor $\gamma$, depth $d$, branching factor $C$}
	\KwOut{estimated value $V$}
	\If {d = 0} {
		$V \leftarrow 0$
	}
	\Else {
		\For {$a \in A$} {
			$Q_a \leftarrow R_{sa}$\\
			\For {$C$ times} {
				$s' \sim P_{sa}$\\
				$V' \leftarrow \mbox{Sparse~Sampling~Recursion}(s', P, R, \gamma, d-1, C)$\\
				$Q_a \leftarrow Q_a + \frac {\gamma V'} C$
			}
		}
		$V \leftarrow \max_a Q_a$
	}
	\Return $V$
\end{algorithm}

\alg{Sparse~Sampling}~\cite{kearns99}, outlined in Algorithm~\ref{alg:ss}, is a Monte-carlo tree search ({\bf MCTS}) algorithm \note{not really} for value estimation on MDPs. \note{why not have the Bayesian sparse sampling section as a subsection here?} This algorithm works by performing exhaustive search, using Monte-carlo sampling to estimate transition probabilities. \note{no, I'd call it monte carlo tree *construction*}

Many value-estimation algorithms, especially those preceding the publication of \alg{Sparse~Sampling}, have a strong linear or super-linear dependence on the number of states in the MDP. This dependence often arises as a result of the algorithm's attempt to estimate the value of all states in the MDP.

\note{explain why you've chosen this level of detail for SS.}

\alg{Sparse~Sampling} reduces the dependence on the number of states by only considering states that are likely to be reached from the state whose value is being estimated. This state is the root of a search tree. Each node in the tree is one particular state in the MDP (though more than one node can share a single state).

This algorithm makes use of a next-state generator $P$ to generate transition samples. If the runtime required to sample from $P_{sa}$ does not depend on the number of states, then \alg{Sparse~Sampling}'s runtime will also not depend on the number of states.

While this sampling runtime condition will not hold true for general MDPs, it makes practical sense for many situations. For example, in a continuous-state MDP (infinite number of states) that uses an offset drawn from a normal distribution to determine the next state, the amount of time required to sample from that normal distribution will depend only on the number of state features, rather than the number of states. Also, in a gridworld, the next state is often chosen from those states adjacent to the current state, and can be sampled very quickly.

\subsection{Algorithm}


The \alg{Sparse~Sampling} algorithm works by sampling a fixed number $C$ of next-states for each action, when taken from the root. The value of those states is estimated recursively and with a search-depth of one less than the current search depth. As a base case, if the current search depth is zero, the value is approximated by $0$.

The algorithm must be provided a probability of success $1-\delta$, required accuracy $\epsilon$ and discount factor $\gamma$. These parameters inform \alg{Sparse~Sampling}'s choice of the parameter $C$ and the desired search depth.

The leaf nodes in the search tree, those whose value is approximated by $0$, are too far in the future, according to the discount factor $\gamma$ and accuracy constraint $\epsilon$, to have a significant affect on the value of the root.

\note{perhaps work through an example of what the bounds suggest for C.  Or maybe just say that it's insanely impractical.}


\subsection{Proof of accuracy}

Because later work in this dissertation depends on it, in this section, we present a proof that \alg{Sparse~Sampling} will make an $\epsilon$--accurate value estimation of the root state with probability at least $1-\delta$.

\note{I do not think this chapter wants proofs.  I'd suggest a background section instead of a related work section.}

Let $V^*(s)$ be the true value of state $s$. Let $Q^*(s,a)$ be the true Q--value of performing action $a$ in state $s$. Let $U^*(s,a) = R_{sa}+\gamma\frac 1 C \sum_{i=1}^C V^*(s_i)$, where $s_i\sim P_{sa}$, be a one-step approximation of $Q^*(s,a)$.

\subsubsection{\alg{Sparse~Sampling} Accuracy Lemma} With high probability, $Q^*(s,a)$ is close to $U^*(s,a)$.
\begin{eqnarray}
|Q^*(s,a)-U^*(s,a)| &=& |R_{sa}+\gamma E_{s'\sim P_{sa}}[V^*(s')] - R_{sa} - \gamma \frac 1 C \sum_{i=1}^C V^*(s_i)|\\
&=& \gamma |E_{s'\sim P_{sa}}[V^*(s')] - \frac 1 C \sum_{i=1}^C V^*(s_i)|
\end{eqnarray}
Claim: $P(|E_{s'\sim P_{sa}}[V^*(s')] - \bar{V^*_C}| \leq \lambda) \geq 1-e^{-\lambda^2 C / {\Vmax^2}}$.

A version of the Chernoff bound states that, for a random variable $X\in[\alpha,\beta]$,
$$P(|E[X] - \bar{X_n}| \leq \eta) \geq 1-2e^{\eta^2 n / 2 b}, b=\frac{(\alpha-\beta)^2}4.$$
In our case, $\eta=\lambda$ and $b=\Vmax^2$.\note{qed symbol}

\note{to use the QED symbol, you should state the result first---not just "close", but what is the bound?  By the way, I use Box.}

Let $Q^n$ and $V^n$ be the estimated Q-values and values $n$ steps from the horizon:
\begin{eqnarray}
Q^n(s,a)&=&R_{sa}+\gamma \frac 1 C \sum_{i=1}^C V^{n-1}(s_i),\\
V^n(s) &=& \max_a Q^n(s,a),\\
Q^0(s,a)&=&0.
\end{eqnarray}

Let $\alpha_n$ be a parameter that will be used to bound the difference between $Q^*$ and $Q^n$:
\note{hang on… where does this expression come from? (first eq)}
\begin{eqnarray}
\alpha_{n+1}&=&\gamma(\lambda + \alpha_n),\\
\alpha_0&=&\Vmax.
\end{eqnarray}
Leading us to the conclusion that
\begin{eqnarray}
\alpha_H &=& \left(\sum_{i=1}^H \gamma^i \lambda\right)+\gamma^H\Vmax\\
 &\leq& \frac \lambda {1-\gamma}+\gamma^H\Vmax.
\end{eqnarray} \note{I think a little more talking about what you are doing and what you did is in order here.  What happened to U*?  Don't we need delta?  The union bound?}

\subsubsection{\alg{Sparse~Sampling} Probability Lemma} With high probability, $Q^*(s,a)$ is close to $Q^n(s,a)$:

\begin{eqnarray}
|Q^*(s,a)-Q^n(s,a)| &=& \gamma\left|E_{s'\sim P_{sa}}[V^*(s')] - \frac 1 C \sum_i V^{n-1}(s_i)\right| \nonumber\\
&\leq& \gamma \left(\left|E_{s'\sim P_{sa}}[V^*(s')] - \frac 1 C \sum_i V^*(s_i)\right| + \left|\frac 1 C \sum_i V^*(s_i) - \frac 1 C \sum_i V^{n-1}(s_i)\right|\right)\\
&\leq& \gamma (\lambda + \alpha_n)\\
&\leq& \alpha_{n+1}.
\end{eqnarray}

This inequality depends on the fact that
$$\forall_{s,a}\left|E_{s'\sim P_{sa}}[V^*(s')] - \frac 1 C \sum_i V^*(s_i)\right|\leq \lambda,$$
which needs to be true for each one of $(kC)^H$ estimates, where $k$ is the number of actions and $H$ is the height of the sampled tree. The likelihood of these all happening at the same time is
$$1-(kC)^H\exp\left(-\frac{\lambda^2 C}{\Vmax^2}\right).$$ \note{QED?}

\subsubsection{Values for Parameters}

We can now bound 
$$|Q^*(s,a)-Q^H(s,a)|\leq \alpha_H \leq \gamma^H\Vmax + \frac{\lambda}{1-\gamma}.$$

Let $H=\log_{\gamma}(\lambda/\Vmax)$. Then,
\begin{eqnarray}
|Q^*(s,a)-Q^H(s,a)|&\leq& \lambda + \frac{\lambda}{1-\gamma}\\
&\leq& \frac{2\lambda-\lambda\gamma}{1-\gamma} \\
&\leq& \frac{2\lambda}{1-\gamma}.
\end{eqnarray}

Let $\lambda = \frac {\epsilon(1-\gamma)^2}{4}$. Then
$$|Q^*(s,a)-Q^H(s,a)| \leq \frac {\epsilon(1-\gamma)}2.$$

\note{always try to end with some kind of recap.  in case we fell off the wagon, what just happened?}



\section{Upper Confidence bounds on Trees}

\begin{algorithm}[tb]
	\caption{$\mbox{UCT}(s, P, R, \gamma, C)$}
	\label{alg:uct}
	\KwIn{initial state $s$, next-state sampler $P$, reward function $R$, discount factor $\gamma$, {\bf UCB} parameter $C$}
	\KwOut{estimated value $V$}
	$\forall_{s} ~ n_{s} \leftarrow 0$\\
	$\forall_{s} ~ V_{s} \leftarrow 0$\\
	$\forall_{s,a} ~ n_{s,a} \leftarrow 0$\\
	$\forall_{s,a} ~ Q_{s,a} \leftarrow 0$\\

	\If {$s$ is terminal} {
		$V_s \leftarrow 0$\\
		\Return $V_s$
	}
	\If {$n_s \neq 0$} {
		$\forall_a ~ B_{s,a} \leftarrow 2 C \sqrt{\frac{\log n_{s}}{n_{s,a}}}$\\
		$a \leftarrow \mbox{argmax}_a ~ Q_{s,a}+B_{s,a}$
	}
	\Else {
		$a \leftarrow \mbox{randomly chosen action}$
	}
	$s' \leftarrow P(s,a)$\\
	$r \leftarrow R(s,a)$\\
	$V_{s'} \leftarrow \mbox{UCT}(s', P, R, \gamma, C)$\\
	$\hat Q_{s,a} \leftarrow r+\gamma v_{s'}$\\
	$Q_{s,a} \leftarrow (n_{s,a}Q_{s,a}+\hat Q_{s,a})/(n_{s,a}+1)$\\
	$n_s \leftarrow n_s+1$\\
	$n_{s,a} \leftarrow n_{s,a}+1$\\
	$V_s \leftarrow \max_a Q_{s,a}$\\
	\Return $V_s$
\end{algorithm}

{\bf Upper Confidence bounds on Trees}~\cite{kocsis06}, or {\bf UCT}, is a widely-used rollout-based {\bf MCTS} value-estimation algorithm. {\bf UCT} is an application of the bandit algorithm {\bf Upper Confidence Bounds}~\cite{auer02}, or {\bf UCB}, to tree search. There are multiple variants to the details of how {\bf UCT} can work, and since none of them relate directly to the work presented in this disseration, only one simple version of {\bf UCT} will be discussed.

The {\bf UTC} algorithm works by running a series of roll-outs through the state space, and making value estimates based on averages of observed returns. The roll-outs follow trajectories generated by applying some exploration policy to the next-state sampler $P$. Once each trajectory has finished, the total discounted return experienced by each state-action pair is averaged with its previously observed returns.

The key to {\bf UCT} is how it chooses its rollout policy. Variations exist, but the part they all have in common is the use of {\bf UCB} for choosing actions in states that have been visited before.

The bandit algorithm {\bf UCB} mixes exploration and exploitation. The algorithm will always choose the arm that has the greatest sum of average observed reward and a bonus term. The bonus term is a function of the number of times the arm in question has been pulled before, and how many chances the algorithm has had to pull an arm in total.

The bonus term used by {\bf UCB} for arm $i$ is $2 C \sqrt{\frac{\log t}{n_i}}$, where $t$ is the total number of pulls on any arm, $n_i$ is the number of times arm $i$ has been pulled, and $C$ is a constant parameter that can be tuned. This bonus term has the nice properties of going to zero as $t$ goes to infinity (allowing {\bf UCB} to rely completely on average observed reward), and being infinite when $n_i$ is zero (forcing {\bf UCB} to try each action at least once).

\note{the depth of explanation in these later algorithms is good.  consider expanding some of the earlier algorithms, as appropriate.}

{\bf UCT} treats every possible state as a separate bandit algorithm, where each action represents a different bandit arm. As it visits a state again and again in its roll-outs, the {\bf UCB} strategy will push it towards areas of the state-space that are either less explored or have had more favorable returns. This algorithm is also extremely computationally efficient, assuming next-states and rewards can be sampled quickly.

\section{Forward Search Sparse Sampling}

{\bf Forward Search Sparse Sampling}~\cite{walsh10}, or \alg{FSSS}, is another MCTS planner that preferentially expands the search tree through the use of rollouts. It is outlined in Algorithm~\ref{alg:fs3}. Unlike either {\bf Bayesian Sparse Sampling} or {\bf UCT}, it retains the attractive guarantees of the original \alg{Sparse~Sampling} algorithm. \alg{FSSS} also maintains hard upper and lower bounds on the values for each state and action so as to direct the rollouts; actions are chosen greedily according to the upper bound on the value, and the next state is chosen such that it is the most uncertain of the available candidates (according to the difference in its upper and lower bounds).

\alg{FSSS} will find the action to take from a given state $s_0$, the root of the search tree.  The tree is expanded by running $t$ trajectories, or rollouts, of length $d$. There are theoretically justified ways to choose $t$ and $d$, but in practical applications they are knobs used to balance computational overhead and accuracy. To run a single rollout, the agent will call Algorithm~\ref{alg:fs3-rollout}, $\mbox{FSSS-Rollout}(s_0, d, 0, M)$.
%, $T$ times. 
The values $U_d(s)$ and $L_d(s)$ are the upper and lower bounds on the value of the node for state $s$ at depth $d$, respectively. Each time a rollout is performed, the tree will be expanded. After at most $(AC)^d$ rollouts are finished (but often less in practice), \alg{FSSS} will have expanded the tree as much as is possibly useful, and will agree with the action chosen by \alg{Sparse~Sampling}. Thus, \alg{FSSS} could be viewed as an anytime version of SS that uses pruning to speed its calculation.

The fact that \alg{FSSS} maintains upper bounds on the values for each state it considers will be useful when an optimistic planner is required. It is often the case that accurately finding (or even approximating) a state's value will end up being very difficult or intractable. \alg{FSSS} promises that, with high probability, the true value for a given state will be somewhere in between $U(s)$ and $L(s)$. Maintaining this range of uncertainty can be useful for the algorithm employing \alg{FSSS} as a planning algorithm, as we shall see in Chapter~\ref{chap:bfs3}.

\begin{algorithm}[tb]
	\caption{$\mbox{FSSS}(s, d, t, M)$}
	\label{alg:fs3}
	\KwIn{state $s$, max depth $d$, \#trajectories $t$, MDP $M$}
	\KwOut{estimated value for state $s$}

	\For {$t$ times} {
		$\mbox{FSSS-Rollout}(s, d, 0, M)$
	}
	{\bf ZZZ pp: Should FSSS-Rollout assign Ud(s,a)?}
	$\hat V(s) \leftarrow \max_a U_d(s, a)$\\
	\Return $\hat V(s)$
\end{algorithm}

\begin{algorithm}[tb]
	\caption{$\mbox{FSSS-Rollout}(s, d, l, M)$}
	\label{alg:fs3-rollout}
	\KwIn{state $s$, max depth $d$, current depth $l$, MDP $M$}
	\If{$\mbox{Terminal}(s)$}{
		$U_d(s)=L_d(s)=0$\\
		\Return
	}
	\If {$d=l$} {
		\Return
	}
	\If {$\neg\mbox{Visited}_d(s)$} {
		$\mbox{Visited}_d(s) \leftarrow \mbox{true}$\\
		\ForEach {$a \in A$} {
			$R_d(s,a),\mbox{Count}_d(s,a,s'),\mbox{Children}_d(s,a)$\\
			\ \ \ $\leftarrow 0, 0, \{\}$\\
			\For {$C$ times} {
				$s', r \sim T_M(s, a), R_M(s,a)$ \\
				$\mbox{Count}_d(s,a,s') \leftarrow \mbox{Count}_d(s,a,s') + 1$ \\
				$\mbox{Children}_d(s,a) \leftarrow \mbox{Children}_d(s,a) \cup \{s'\}$ \\
				$R_d(s,a) \leftarrow R_d(s, a)+r/C$\\
				\If {$\neg\mbox{Visited}_{d+1}(s')$} {
					 $U_{d+1}(s'), L_{d+1}(s') \leftarrow \Vmax, \Vmin$
				}
			}
		}
		$\mbox{Bellman-backup}(s, d)$
	}
	$a \leftarrow \argmax_a U_d(s,a)$\\
	$s' \leftarrow \argmax_{s'} (U_{d+1}(s')-L_{d+1}(s')) \cdot \mbox{Count}_d(s,a,s')$\\
	$\mbox{FSSS-Rollout}(s', d, l+1, M)$\\
	$\mbox{Bellman-backup}(s, d)$\\
	\Return
\end{algorithm}


\note{I'd like to see Finale's stuff cited (as it is a prominent example of combining Bayesian priors with sequential decision making).  Even more importantly, the Oregon work should be described, as it is one of the only other approaches that uses general priors in RL.}

%
\ifperchapterbib%
For the convenience of the reader, a list of references is provided at the end of each chapter (where applicable).
\ifendbib%
%A bibliography containing all cited references is included at the \hyperref[sec:bibliography]{end of the dissertation}.
\else\fi% end ifendbib
%\cbend%
\else\fi% end ifperchapterbib
