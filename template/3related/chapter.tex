
In this chapter, we discuss important related work that will influence, directly and indirectly, later contributions. \note{Can you say briefly what dimensions of relatedness you are using?  How are you deciding what to include and what not to include?} Along with algorithms of general interest to a student of model-based Bayesian reinforcement-learning, there are
\reply{
\begin{itemize}
\item existing model-based RL algorithms used for comparisons in Chapter~\ref{sec:experiments}~\cite{brafman03,jong07}.
\item algorithms that make use of a flexible prior~\cite{strens00,wang05,sorg10,araya2012near},
\item model-based Bayesian RL algorithms with PAC-BAMDP guarantees~\cite{kolter09,sorg10,araya2012near}.
\item the \alg{FSSS} algorithm, which is used as a building block in Chapter~\ref{sec:relmbbrl}, and several other tree search planners~\cite{kearns99,kocsis06}.
\end{itemize}
}
\section{RMAX}

The \alg{RMAX}~\cite{brafman03} algorithm is a simple and robust method to balance exploration and exploitation in the reinforcement-learning setting. The original algorithm addresses only discrete state- and action-spaces, but the ideas apply more generally~\cite{nouri09,jong07}.

To effectively use \alg{RMAX}, the agent designer needs a metric to determine whether or not a particular state-action pair is \emph{known}. In the discrete state and action case, the agent can count how many times it has tried a given action pair. If that number exceeds a predetermined threshold, the given pair is \emph{known}. Otherwise, it is \emph{unknown}.

To guide the \alg{RMAX} agent, a model is constructed from the agent's observations, with unknown state-action pairs transitioning only to a nirvana state, or a state with the highest possible value. Known states have the transition dynamics given by the MLE. The agent then acts greedily according to this model.

Clamping unknown states at very high value will draw the agent towards these states if there is any chance that visiting those states is the optimal thing to do.

Because the model maintained by \alg{RMAX} is accurate (when the state is known), optimistic (when the state is unknown) and there is a bounded number of times that the agent can be surprised (because there is a finite number of state-action pairs that can become known), the authors were able to show that the algorithm is PAC-MDP. That is, it will make only a small number of suboptimal decisions over the course of its lifetime.

\subsection{Potential-based RMAX}

As it stands, the \alg{RMAX} algorithm cannot make use of any prior knowledge. All unknown states are assigned the highest possible value, and the agent is drawn to each of them in a way that depends only on how quickly it can reach them.

The \alg{Potential-based shaping RMAX}~\cite{asmuth08} algorithm addresses this deficiency by incorporating a potential function $\phi(s)$. The agent will perform well if this potential function is \emph{admissible}. That is, $\forall_s ~ V(s) \leq \phi(s)$; the potential function's value for a given state must never be less than the true value for the state. This admissibility requirement is common in heuristic-based search algorithms, such as A*~\cite{russell1995artificial}.

This algorithm works by assigning unknown states a value of $\phi(s)$, rather than $\Vmax$, the highest possible value. Setting $\phi(s)=\Vmax$ is allowed, since $\Vmax$ automatically satisfies the admissibility requirement, but lower values result in faster convergence.

On the downside, this algorithm is very conservative, like the rest of the \alg{RMAX} family. Requiring a fixed number of visits to a state-action pair before it becomes known means a lot of exploration steps if the heuristic is too optimistic, some of which might not be necessary in practice.


\subsection{Fitted-RMAX}

Instance-based methods represent functions by a set of examples and use a predefined similarity function to generalize to novel inputs. In the reinforcement-learning setting, an instance is the tuple $\langle s, a, s', r \rangle$, or the combination of state, action, next state and reward.

Over the course of its lifetime, the agent gathers a large set of $\langle s, a\rangle \rightarrow \langle s', r \rangle$ mappings. When it observes a new state-action pair, it can compare it to those it has seen in the past and predict a next-state and reward that fits with those already observed.

The ideas behind instance-based model learning can be combined with the \alg{RMAX} algorithm to create {\bf Fitted-RMAX}~\cite{jong07}. {\bf Fitted-RMAX} uses a Gaussian kernel to determine the similarity between two states:
\begin{eqnarray}
d(s_1,s_2)&=&\exp\left(\frac{-||s_1 - s_2||^2_2}{b}\right),
\end{eqnarray}
where $b$ is a \emph{width} parameter used to determine how much distance affects the similarity between two states.

The possible outcomes and rewards are then taken from a weighted average of those seen before when taking the action in question, where each possibility $\langle o_i, r_i \rangle$ is weighted by $d(s_t, s_i)$, where $s_t$ is the state from which the agent is considering taking an action.

Since this algorithm is applied in continuous domains, the standard definition of knownness that uses visit counts cannot be used.  Instead, a distance metric needs to be incorporated. The sum of the similarity scores between the state in question and all previously experienced states is taken and compared to a threshold $B$, over which the knownness metric declares a state to be known. Unknown states are then assigned maximum value, according to the rules of \alg{RMAX}.

Once a policy is found for the {\bf Fitted-RMAX} model, using some existing continuous-state planner, the agent will be drawn to explore states that are either unknown or have high value, like the original \alg{RMAX} on which it is based.



\section{Bayes-adaptive Markov decision processes}

\note{jta: this is now part of the intro as well, and since this section doesn't describe an algorithm, maybe delete it}

The Bayes-adaptive MDP, or BAMDP~\cite{duff03}, is an MDP whose optimal policy is the Bayes-optimal policy for some other unknown MDP that is drawn from a known prior. Every state in the BAMDP is a belief-state composed of a history, or the set of observations made by the agent so far, and the concrete state that the agent is currently in.

The transition function of the BAMDP is defined as follows:
\begin{eqnarray}
\label{rel:bamdp:prob}
P(\langle s', h + (s,a,s',r)\rangle, r | \langle s, h\rangle, a) =& \int_M P(s', r | s, a, M)\phi(M|h) dM,
\end{eqnarray}
\note{pp: Why is the distribution over h?  Should it be conditioned on h only?}
where $s$ and $s'$ are \emph{concrete} states from the unknown MDP, $\langle s, h\rangle$ is a belief-state, in the BAMDP, where the agent begins, and is composed of the concrete state $s$ and history $h$. The value $h$ is the summary of all transitions observed by the agent over the course of its lifetime. Here, they are represented by $(s, a, s',r)$ tuples detailing the beginning and ending concrete states, the action taken and the reward received.

The likelihood of the transition in the BAMDP is taken by integrating out the underlying and unknown ``true'' MDP, according to MDP's posterior distribution. If a next-state sampler is needed, rather than an exact transition probability, the process is to first sample an MDP $M$ from the posterior distribution $\phi(M|h)$, and then sample the next state $s'$ and reward $r$ from $M$.

By constructing the BAMDP according to Equation~\ref{rel:bamdp:prob}, the optimal policy is identical to the Bayes-optimal policy in the learning setting. This fact follows because the succession of transition likelihoods is exactly the same as those indicated in the Bayesian model\footnote{See Chapter~\ref{sec:intro}, Section~\ref{sec:intro:bayes-opt} for a full explanation of why optimal in the BAMDP is Bayes-optimal while learning the MDP.}. \note{ml: Can you sum up why that's important? jta: added footnote}

Unfortunately, the BAMDP has potentially an infinite number of states, since each possible history maps to a different belief state. However, as the number of observations from a particular concrete state and using a particular action grows large, the distribution over the possible next concrete states converges, allowing a clever algorithm designer to collect large sets of belief states together into equivalent classes, for the purposes of planning. \note{Has that been done?  What is the status of this algorithm?} \reply{An easy way to make this optimization is to not adjust the posterior when an observation is made for a particular state-action pair $s,a$ if that pair has been observed at least $B$ times in the past, for a choice of $B$ sensible for the prior. This trick can be used in any model-based Bayesian RL algorithm that uses a posterior sampler.}



\section{Bayesian Dynamic Programming}

\begin{algorithm}[tb]
	\caption{$\mbox{Bayesian~DP}(s, \Phi, K)$}
	\label{alg:bdp}
	\KwIn{initial state $s$, prior $\Phi$, \#steps $K$}
	$O \leftarrow \{\}$\\
	\For {ever} {
		$M \sim \Phi|O$ \bf{ZZZ} pp: Be more precise by making it clear that this is a distribution over phi given O.\\
		$\pi \leftarrow \mbox{solve}(M)$\\
		\For {$K$ times} {
			$a \leftarrow \pi(s)$\\
			$s', r \leftarrow \mbox{perform}(a)$\\
			$O \leftarrow O \cup \{(s, a, s', r)\} $\\
			$s \leftarrow s'$
		}
	}
\end{algorithm}

Bayesian Dynamic Programming~\cite{strens00}, or \alg{Bayesian~DP}, is one of the first model-based reinforcement learning algorithms to take advantage of posterior sampling. The agent requires a model prior $\phi$ and a parameter $K$, which tells the agent how many steps should be taken between samples.

Algorithm~\ref{alg:bdp} lays out the \alg{Bayesian~DP} algorithm. The agent starts in some initial state with no information about the environment except that which can be gleaned from the prior $\Phi$. Before taking any actions, the agent samples a model $M$ from the posterior $\Phi|O$, where $\Phi$ is the model prior and $O$ is the set of observations recorded so far. Once $M$ is sampled from the posterior, it is solved optimally by whatever planning algorithm the designer prefers. The resulting policy $\pi$ is acted upon for the next $K$ steps, and each step is recorded and added to the set of observations. After $K$ actions have been performed, a new model is sampled from the updated posterior, and the process is repeated until the end of the experiment.

The act of periodically sampling from the posterior is intended to give the agent glimpses of possible worlds. Some of the times a sample is drawn, a state that the agent has not visited much will appear better than it actually is, drawing the agent towards this relatively unknown state. On the other hand, some times that unknown state will appear to have a very low value, causing the agent to avoid it. State-action pairs that the agent has experienced many times in the past will have relatively stable dynamics, and will usually have a consistent value with respect to its neighboring states. As a result, this approach strikes a balance between exploration of states in which it is uncertain and exploitation of states in which it is confident.

The fact that some samples will make an unknown state appear attractive and other samples will make the same unknown state appear unattractive requires \alg{Bayesian~DP} to be careful about when and how often it samples new models. Too long between samples, and the agent is acting based on stale information.  Too short between samples and the agent can end up dithering, as explained next.

The parameter $K$, which is the number of steps taken between samples, should be chosen to approximate the length of a trial (in situations where there is a goal state) or the width of the MDP (maximum expected number of steps to get from any given state to any other given state) when there is an infinite horizon.

The idea here is that the agent should be able to reach any particular state it wants before the next sample is taken and the agent might change its mind about where it is trying to go. Consider a simple \env{chain} MDP, where only the two states on the end are unknown. In a given posterior sample, one of the two ends may have a higher apparent value than the other. In the next sample, the relative values of these two states may switch. If a new sample is taken before the agent can reach one of the ends (and therefore learn about its dynamics), thrashing can occur, resulting in an exponential number of steps (relative to the number of states) before knowledge is gained.

To understand how thrashing can cause an exponential number of sub-optimal steps, consider an MDP with one root state and $N$ ``chains'' leading off from that root state. At the end of one of these chains is the goal, and the prior does not give preference to which chain might have that goal. At the root, the agent can choose which chain to travel, but after that is limited to two choices: continue or reset. Everything about the MDP is known except the identity of the chain containing the high-valued goal. As a result, the posterior does not change except when the agent reaches the end of a chain that it has not reached before. Since the prior has no indication of which chain has the goal, each time a sample is drawn the goal is randomly put at the end of one of the chains. With \alg{Bayesian~DP}, if the agent is already on this chain, it can continue getting closer to the goal. If the agent is on the wrong chain, it needs to reset and head down the correct one. 

If the length of each of the chains is $L$, this requires $L/K$ identical guesses in a row in order for the agent to either get the goal or remove that chain from the posterior. With $N$ unknown chains remaining, the likelihood of this event happening for any $\frac L K$ consecutive posterior samples is $(\frac 1 N)^\frac L K$. Briefly, consider the problem of counting the number of flips of a biased coin before $M$ consecutive heads turn up, where each flip is heads with probability $p$. The likelihood of flipping $M$ heads in a row is $p^M$. At the beginning, or immediately following a tails, there are $M$ different events that can happen, each with different likelihoods. There are $M-1$ versions of flipping $T$ heads and then a tails, for $0\leq T < M$ (a failure), each with probability $p^T(1-p)$ and taking $T$ flips, and there is flipping $M$ heads (a success), with probability $p^M$ and taking $M$ flips. Since the expected number of failures before a success is $1/(p^M)$, we can count the total number of expected steps by multiplying $1/(p^M)$ by the expected number of steps in a failure, which is
\begin{eqnarray}
E[T] &=& \sum_{T=0}^{M-1} P(T) T,\\
 &=& \sum_{T=0}^{M-1} p^T (1-p) T,\\
 &=&  (1-p)p \sum_{T=0}^{M-1} p^{T-1} T,\\
 &=&  (1-p)p \sum_{T=0}^{M-1} \frac {d p^T} {d p},\\
 &=&  (1-p)p \frac {d \sum_{T=0}^{M-1} p^T} {d p},\\
 &=&  (1-p)p \frac {d \frac {1-p^M}{1-p}} {d p},\\
 &=&  (1-p)p \left[ (1-p)(1-p^M)-\frac{Mp^{M-1}}{1-p}\right],\\
 &=&  p(1-p)^2(1-p^M)-Mp^{M}.
\end{eqnarray}
Returning to the $N$-chain MDP, if we consider $p=(\frac 1 N)^\frac L K$ and $M=\frac L K$, the expected number of samples drawn (coins flipped) in a failure will be $E[T]+1$ (the first sample is what we choose to think of as heads - the others then need to match it). With an expected $(\frac 1 N)^{-\frac L K}$ failures before the first success, that gives an expected 
\begin{eqnarray}
\lefteqn{E[\mbox{number of failures}]E[\mbox{steps per failure}]+E[\mbox{steps per success}]}\\
 &=& \left(\frac 1 N\right)^{-\frac L K} (p(1-p)^2(1-p^{\frac L K})-{\frac L K}p^{\frac L K}+1) + \frac L K
\end{eqnarray} samples before reaching the end of one of the chains. Ignoring the smaller pieces, the $(\frac 1 N)^{-\frac L K}$ factor is exponential with respect to the length of the chain.

Spacing out the MDP samples temporally allows the agent to visit at least one unknown (and optimistically sampled) state every $K$ steps (assuming that any of the unknown states are optimistic in the posterior sample for those $K$ steps). Choosing a $K$ that is at least as great as the expected number of steps to get from any state in the MDP to any other state in the MDP alleviates the thrashing issue seen in the $N$-chains example above. With $K=L$, $(\frac 1 N)^{-\frac L K} \rightarrow N$, and there is no exponential factor in the expectation.


\section{Multi-Task Reinforcement Learning}

Multi-Task Reinforcement Learning, or MTRL, considers the scenario where an agent is given a sequence of environments, each of which is drawn i.i.d. from some prior distribution. The \alg{Hierarchical Bayesian MTRL} algorithm~\cite{wilson07}, or \alg{HBMTRL} assumes the following generative process for creating sequences of MDPs:
\begin{eqnarray}
\label{sec:rel:hbmtrl}
C &\sim& \mbox{CRP}(\alpha),\\
\theta_c &\sim& \Theta,\\
m_i &\sim& \phi(\theta_{C_i}),
\end{eqnarray}
where $\alpha$ is the CRP concentration parameter, $C$ is the assignment of MDPs to ``classes'' with $C_i$ being the class of MDP $m_i$, $\Theta$ is a distribution over MDP hyper-parameters, $\theta_c$ is the hyper-parameter used for MDPs in class $c$, $\phi$ is some distribution over MDPs parameterized by an instance of $\theta$, and $m_i$ is an MDP in the sequence.

With this approach, the hyper-parameter distribution $\Theta$ and the MDP distribution family $\phi$ can be anything - the algorithm itself has no constraints on what may be used.

The process \alg{HBMTRL} uses is similar to that of \alg{Bayesian DP}~\cite{strens00}, except that instead of sampling a single model from the posterior and acting according to its optimal policy, \alg{HBMTRL} will sample several MDPs and discard all of them but the most likely. This has the effect of sampling from a peaked version of the posterior, where the likelihood ratios between two samples grows more severe. As the size of the sample set grows, the resulting version of the posterior becomes more and more peaked. In the limit as the size of sample set goes to infinity, this process will choose the mode of the posterior.

This algorithm is an example of using model-based Bayesian RL for transfer learning, where experience gathered in one MDP can inform an agent's behavior in another. The model in Equation~\ref{sec:rel:hbmtrl} indicates that all MDPs can be divided into groups which share some basic qualities, represented by $\theta_i$.

In the parlance of this dissertation, \alg{HBMTRL} is both an algorithm and a family of priors. The algorithm is the process of sampling from the peaked posterior and acting according to the optimal policy of the result. The family of priors is those priors that use a Dirichlet process mixture model to sample MDPs.


\section{BEETLE}

Bayesian Exploration Exploitation Tradeoff in LEarning, or \alg{BEETLE}~\cite{poupart06} treats the task of Bayes-optimal behavior as optimal behavior in a POMDP whose hidden state is the MDP describing the transition probabilities between the observed states.

This algorithm assumes the Flat Dirichlet Multinomial prior, where each of the MDP's next-state distributions are drawn from a Dirichlet distribution.

The POMDP's state-space is $S \times \Theta$, where $\Theta$ is the set of next-state likelihood vectors, describing the entire dynamics of the MPD. The POMDP's action space is the same as that of the underlying MDP's. The transition function is $P(\langle s', \theta' \rangle | \langle s,\theta\rangle, a) = \theta_{s,a}^{s'}\delta(\theta,\theta')$, where $\delta$ is the Kronecker delta. The set of observations is the set of states from the underlying MDP. And, the reward function is known.

Since the Dirichlet distribution is conjugate prior to the multionimial distribution, belief monitoring is straightforward.

 \alg{BEETLE} is a point-based value iteration method for this sort of POMDP, and for every iteration it will prune the number of belief-states that it is considering in order to mitigate the natural intractibility of POMDP planning.

%\note{pp: We use Dirichlets to express distributions over the model parameters, but we don't draw any samples.  All integrals are computed exactly.}

%\note{Given that Pascal is on the committee, it sure seems like this section should be extensive.  Currently, it's the shortest one.}




\section{Bayesian Exploration Bonus}

The \alg{BEB}~\cite{kolter09} algorithm is a computationally efficient method to achieve near Bayes-optimal behavior in discrete state- and action-spaces when using a Dirichlet prior.

The algorithm uses a flat Dirichlet multinomial (FDM) prior, which states that
\begin{eqnarray}
\theta^{s,a}&\sim&Dirichlet(\alpha),\\
P(s'|s,a)&=&\theta^{s,a}_{s'},
\end{eqnarray}
where $\alpha$ is a tuning parameter that correlates with variance in the next-state distribution.

\alg{BEB} assumes the reward function is known.

To drive exploration, \alg{BEB} adds an internal bonus $B(s,a)$ to the reward received when taking action $a$ from state $s$. It acts greedily according to $\hat Q(s,a)$:
\begin{eqnarray}
\hat Q(s,a)&=&R(s,a)+B(s,a)+\gamma \sum_{s'} P(s'|s,a)\hat V(s'),\\
\hat V(s)&=& \max_a \hat Q(s, a),\\
\label{eqn:rel:beb:bonus}
B(s,a)&=&\frac\beta{1+n(s,a)},
\end{eqnarray}
where $n(s,a)$ is the number of times that action $a$ has been taken from state $s$.

This algorithm is similar to Model-Based Interval Estimation (\alg{MBIE})~\cite{strehl06}, except that \alg{MBIE} uses the square root of the visit count in its bonus. This difference causes the bonus term used by \alg{MBIE} to decay more slowly resulting in slower convergence, and that extra conservativity allows \alg{MBIE} to be a PAC-MDP algorithm, where \alg{BEB} is a near Bayes-optimal algorithm when the prior is the flat Dirichlet multinomial.

\section{Variance-based BEB}

Variance-based {\bf BEB}, or {\bf vBEB}~\cite{sorg10}, is an adaptation of {\bf BEB} that can make use of a flexible prior, or one that is not restricted to the flat Dirichlet multinomial.

The {\bf vBEB} algorithm provides a framework for analyzing an arbitrary prior, and creating a bonus term based on the posterior variance for the state-action pair to which the bonus will be applied. In this framework, the flat Dirichlet multinomial gets the bonus term from Equation~\ref{eqn:rel:beb:bonus}, and it will be different for other priors. With the proper bonus selected, \alg{vBEB} is a PAC-BAMDP algorithm.

\section{BOLT}

\alg{Bayesian Optimistic Local Transition}, or \alg{BOLT}~\cite{araya2012near}, makes use of a flexible prior and has PAC-BAMDP guarantees. \alg{BOLT} works by constructing a hyper-model $m'$ from the posterior $\phi|h$, where $\phi$ is the model prior and $h$ is the agent's current history at the time that the hyper-model is constructed. This hyper-model has the same state-space, but an expanded action-space $A' = S \times A$. The transition function of the hyper-model is
\begin{eqnarray}
T_{m'}(s'|s, \langle \hat s, a \rangle) &=& \int_m \phi(m|h+(s,a,\hat s)^{(\eta)})T_m(s'|s,a) d m,
\end{eqnarray}
where $h+(s,a,\hat s)^{(\eta)}$ is the current history augmented with $\eta$ extra hypothetical transitions from $s$ to $\hat s$ when choosing action $a$. As a result, $\phi(m|h+(s,a,\hat s)^{(\eta)})$ is the hypothetical posterior likelihood of $m$ after the extra transitions.

This hyper-model provides a way to introduce optimism into the partially known or unknown MDP. At least one of the states must have high value (otherwise, why bother?), and for a transition function that still has variance in the posterior, creating an action with a higher likelihood of ending up in that state allows the planner to pick and choose which states visits. Unlike \alg{RMAX}- and \alg{BEB}-based algorithms, which augment rewards for state-action pairs with poorly known transition functions, \alg{BOLT} changes the transition function itself to reflect that uncertainty.



\section{Sparse Sampling}

\label{sec:rel:ss}

\begin{algorithm}[tb]
	\caption{$\mbox{Sparse~Sampling}(s, P, R, \gamma, \epsilon, \delta)$}
	\label{alg:ss}
	\KwIn{initial state $s$, next-state sampler $P$, reward function $R$, discount factor $\gamma$, accuracy $\epsilon$, probability of failure $\delta$}
	\KwOut{estimated value $V$}
	$\Rmax \leftarrow \max_{sa}R_{sa}$\\
	$\Vmax \leftarrow \frac{\Rmax}{1-\gamma}$\\
	$\lambda \leftarrow \frac{\epsilon(1-\gamma)^2} 4$\\
	$k \leftarrow \#\mbox{actions}$\\
	$H \leftarrow \left\lceil\log_\gamma(\lambda/\Vmax)\right\rceil$\\
	$C \leftarrow \frac {\Vmax^2}{\lambda^2}\left(2H\log\frac{kH\Vmax^2}{\lambda^2}+\log\frac{\Rmax}{\lambda}\right)$\\
	$V \leftarrow \mbox{Sparse~Sampling~Recursion}(s, P, R, \gamma, H, C)$\\
	\Return $V$
\end{algorithm}

\begin{algorithm}[tb]
	\caption{$\mbox{Sparse~Sampling~Recursion}(s, P, R, \gamma, d, C)$}
	\label{alg:ssr}
	\KwIn{initial state $s$, next-state sampler $P$, reward function $R$, discount factor $\gamma$, depth $d$, branching factor $C$}
	\KwOut{estimated value $V$}
	\If {d = 0} {
		$V \leftarrow 0$
	}
	\Else {
		\For {$a \in A$} {
			$Q_a \leftarrow R_{sa}$\\
			\For {$C$ times} {
				$s' \sim P_{sa}$\\
				$V' \leftarrow \mbox{Sparse~Sampling~Recursion}(s', P, R, \gamma, d-1, C)$\\
				$Q_a \leftarrow Q_a + \frac {\gamma V'} C$
			}
		}
		$V \leftarrow \max_a Q_a$
	}
	\Return $V$
\end{algorithm}

\alg{Sparse~Sampling}~\cite{kearns99}, outlined in Algorithm~\ref{alg:ss}, is a Monte-carlo tree construction algorithm for value estimation on MDPs. This algorithm works by performing exhaustive search, using Monte-carlo sampling to estimate transition probabilities.

Many value-estimation algorithms, especially those preceding the publication of \alg{Sparse~Sampling}, have a strong linear or super-linear dependence on the number of states in the MDP. This dependence often arises as a result of the algorithm's attempt to estimate the value of all states in the MDP.

\alg{Sparse~Sampling} reduces the dependence on the number of states by only considering states that are likely to be reached from the state whose value is being estimated. This state is the root of a search tree. Each node in the tree is one particular state in the MDP (though more than one node can share a single state).

This algorithm makes use of a next-state generator $P$ to generate transition samples. If the runtime required to sample from $P_{sa}$ does not depend on the number of states, then \alg{Sparse~Sampling}'s runtime will also not depend on the number of states.

While this sampling runtime condition will not hold true for general MDPs, it makes practical sense for many situations. For example, in a continuous-state MDP (infinite number of states) that uses an offset drawn from a normal distribution to determine the next state, the amount of time required to sample from that normal distribution will depend only on the number of state features, rather than the number of states. Also, in a gridworld, the next state is often chosen from those states adjacent to the current state, and can be sampled very quickly.

The \alg{Sparse~Sampling} algorithm works by sampling a fixed number $C$ of next-states for each action, when taken from the root. The value of those states is estimated recursively and with a search-depth of one less than the current search depth. As a base case, if the current search depth is zero, the value is approximated by $0$.

The algorithm must be provided a probability of success $1-\delta$, required accuracy $\epsilon$ and discount factor $\gamma$. These parameters inform \alg{Sparse~Sampling}'s choice of the parameter $C$ and the desired search depth.

The leaf nodes in the search tree, those whose value is approximated by $0$, are too far in the future, according to the discount factor $\gamma$ and accuracy constraint $\epsilon$, to have a significant affect on the value of the root.

Usually, the value $C$ needed in order to achieve a particular $\epsilon$ and $\delta$ is too high for any kind of practical use. Simply running \alg{Sparse~Sampling} on the BAMDP, and taking the action that it suggests has the highest value, results in an approximately Bayes-optimal agent. But, since \alg{Sparse~Sampling} would require an enormous amount of computation, algorithms that require less computation (usually at the cost of some number of sub-Bayes-optimal steps) are desireable.

\section{Bayesian Sparse Sampling}

\alg{Bayesian Sparse Sampling}~\cite{wang05} is an adaptation of Sparse Sampling (see Section~\ref{sec:rel:ss}) that uses a model prior and a targetted method of choosing actions while planning.

Where Sparse Sampling is only able to estimate a state's value for a particular concrete model, Bayesian Sparse Sampling works with uncertainty, codified by a provided model prior. Every time a next-state is needed to further build the search tree, a model is sampled from the posterior, and the next-state is sampled from that model. Future next-states in the sub-tree of the newly sampled next-state will include this generated transition in their observation sets, allowing the sub-trees to simulate learning. This process is analogous to \alg{Sparse Sampling} in the BAMDP, except that \alg{Bayesian Sparse Sampling} explores only a small portion of the search tree that \alg{Sparse Sampling} would discover. 

The method of action-selection differs from Sparse Sampling in that rather than trying each action a number of times, Bayesian Sparse Sampling will run a series of roll-outs (simulated trajectories through the BAMDP). To choose the action at each stage in the roll-out, Bayesian Sparse Sampling samples a model from the posterior and finds the optimal action in the sampled model from the current state, and chooses that state, similar to Thompson sampling~\cite{thompson33}.

\note{ml: wait, if it's doing roll outs, why does it need the sparse sampling tree at all? jta: i don't understand the question}

Choosing actions by identifying the optimal action in posterior samples allows Bayesian Sparse Sampling to target its exploration to attempt only actions that have a chance of being optimal. Also, when the posterior has converged to something close to the truth, the sampled model will be consistent and correct, and Bayesian Sparse Sampling will only choose the best action, wasting no samples in its value estimation.

This algorithm has some efficiency issues; it requires sampling from the posterior and solving for the true optimal action in the inner loop. Both posterior sampling and model solving can often be computationally difficult. There are cases where the posterior sampling and model solving can be done efficiently, for instance when the model is a bandit problem with the Beta prior~\cite{wang05}.


\section{Bayesian approaches to acting in POMDPs}

The majority of the work discussed in this dissertation focuses on using priors to help an agent explore and exploit an MDP. Even if that MDP is not known exactly, the agent gets to know exactly what states it visits, and is guaranteed that $T(s'|s,a)$ is constant, if sometimes unknown, throughout the course of the experiment.

When trying to act in a POMDP, the agent only gets to oberve a signal, which is drawn from a distribution and conditioned on the unobserved state. This disconnect makes learning much more difficult.

The use of Bayesian non-parametric techniques is a proven method for learning latent variables in a variety of situations~\cite{blei2003latent,Blei04hierarchicaltopic,asmuth09}, and it makes sense to use them for POMDP inference as well.

The Infinite POMDP~\cite{doshi2009infinite} is an example of using non-parametric techniques for POMDP inference. It uses a CRP to assign specific observations to states, and a Stick-breaking process to create next-state distributions. The CRP and the Stick-breaking process are both different formulations of the Dirichlet process, so the result is a hierarchical Dirichlet process.

The model can be written as follows:
\begin{eqnarray}
T_{s,a} &\sim& \mbox{Stick}(\lambda),\\
\Omega_{s,a} &\sim& H,\\
R_{s,a} &\sim& H_R,\\
S &\sim& \mbox{CRP}(\alpha),\\
s_{t+1} &\sim& T_{S_t,a},\\
o_t &\sim& \Omega_{S_t,a},
\end{eqnarray}
where $\lambda$ and $\alpha$ are concentration hyper-parameters to their respective distributions, $H$ is a prior over different observation distributions, $\Omega_{s,a}$ is the observation distribution when in hidden state $s$ and performing action $a$, $R{s,a}$ is the reward distribution when in hidden state $s$ and performing action $a$, $H_R$ is a prior over different reward distributions, $S$ is the assignment of time-steps to states (so at time $t$, the agent was in state $S_t$), and $o_t$ is the observation made after taking an action during time-step $t$.

This model is very general, and allows the agent designers choose a model for the observations and rewards that fits with the domain they are working with.

\section{Upper Confidence bounds on Trees}

\begin{algorithm}[tb]
	\caption{$\mbox{UCT}(s, P, R, \gamma, C)$}
	\label{alg:uct}
	\KwIn{initial state $s$, next-state sampler $P$, reward function $R$, discount factor $\gamma$, {\bf UCB} parameter $C$}
	\KwOut{estimated value $V$}
	$\forall_{s} ~ n_{s} \leftarrow 0$\\
	$\forall_{s} ~ V_{s} \leftarrow 0$\\
	$\forall_{s,a} ~ n_{s,a} \leftarrow 0$\\
	$\forall_{s,a} ~ Q_{s,a} \leftarrow 0$\\

	\If {$s$ is terminal} {
		$V_s \leftarrow 0$\\
		\Return $V_s$
	}
	\If {$n_s \neq 0$} {
		$\forall_a ~ B_{s,a} \leftarrow 2 C \sqrt{\frac{\log n_{s}}{n_{s,a}}}$\\
		$a \leftarrow \mbox{argmax}_a ~ Q_{s,a}+B_{s,a}$
	}
	\Else {
		$a \leftarrow \mbox{randomly chosen action}$
	}
	$s' \leftarrow P(s,a)$\\
	$r \leftarrow R(s,a)$\\
	$V_{s'} \leftarrow \mbox{UCT}(s', P, R, \gamma, C)$\\
	$\hat Q_{s,a} \leftarrow r+\gamma v_{s'}$\\
	$Q_{s,a} \leftarrow (n_{s,a}Q_{s,a}+\hat Q_{s,a})/(n_{s,a}+1)$\\
	$n_s \leftarrow n_s+1$\\
	$n_{s,a} \leftarrow n_{s,a}+1$\\
	$V_s \leftarrow \max_a Q_{s,a}$\\
	\Return $V_s$
\end{algorithm}

{\bf Upper Confidence bounds on Trees}~\cite{kocsis06}, or {\bf UCT}, is a widely-used rollout-based {\bf MCTS} value-estimation algorithm. {\bf UCT} is an application of the bandit algorithm {\bf Upper Confidence Bounds}~\cite{auer02}, or {\bf UCB}, to tree search. There are multiple variants to the details of how {\bf UCT} can work, and since none of them relate directly to the work presented in this disseration, only one simple version of {\bf UCT} will be discussed.

The {\bf UTC} algorithm works by running a series of roll-outs through the state space, and making value estimates based on averages of observed returns. The roll-outs follow trajectories generated by applying some exploration policy to the next-state sampler $P$. Once each trajectory has finished, the total discounted return experienced by each state-action pair is averaged with its previously observed returns.

The key to {\bf UCT} is how it chooses its rollout policy. Variations exist, but the part they all have in common is the use of {\bf UCB} for choosing actions in states that have been visited before.

The bandit algorithm {\bf UCB} mixes exploration and exploitation. The algorithm will always choose the arm that has the greatest sum of average observed reward and a bonus term. The bonus term is a function of the number of times the arm in question has been pulled before, and how many chances the algorithm has had to pull an arm in total.

The bonus term used by {\bf UCB} for arm $i$ is $2 C \sqrt{\frac{\log t}{n_i}}$, where $t$ is the total number of pulls on any arm, $n_i$ is the number of times arm $i$ has been pulled, and $C$ is a constant parameter that can be tuned. This bonus term has the nice properties of going to zero as $t$ goes to infinity (allowing {\bf UCB} to rely completely on average observed reward), and being infinite when $n_i$ is zero (forcing {\bf UCB} to try each action at least once).

\note{ml: the depth of explanation in these later algorithms is good.  consider expanding some of the earlier algorithms, as appropriate.}

{\bf UCT} treats every possible state as a separate bandit algorithm, where each action represents a different bandit arm. As it visits a state again and again in its roll-outs, the {\bf UCB} strategy will push it towards areas of the state-space that are either less explored or have had more favorable returns. This algorithm is also extremely computationally efficient, assuming next-states and rewards can be sampled quickly.

\section{Forward Search Sparse Sampling}

\label{sec:rel:fsss}

{\bf Forward Search Sparse Sampling}~\cite{walsh10}, or \alg{FSSS}, is another MCTS planner that preferentially expands the search tree through the use of rollouts. It is outlined in Algorithm~\ref{alg:fs3}. Unlike either {\bf Bayesian Sparse Sampling} or {\bf UCT}, it retains the attractive guarantees of the original \alg{Sparse~Sampling} algorithm. \alg{FSSS} also maintains hard upper and lower bounds on the values for each state and action so as to direct the rollouts; actions are chosen greedily according to the upper bound on the value, and the next state is chosen such that it is the most uncertain of the available candidates (according to the difference in its upper and lower bounds).

\alg{FSSS} will find the action to take from a given state $s_0$, the root of the search tree.  The tree is expanded by running $t$ trajectories, or rollouts, of length $d$. There are theoretically justified ways to choose $t$ and $d$, but in practical applications they are knobs used to balance computational overhead and accuracy. To run a single rollout, the agent will call Algorithm~\ref{alg:fs3-rollout}, $\mbox{FSSS-Rollout}(s_0, d, 0, M)$.
%, $T$ times. 
The values $U_d(s)$ and $L_d(s)$ are the upper and lower bounds on the value of the node for state $s$ at depth $d$, respectively. Each time a rollout is performed, the tree will be expanded. After at most $(AC)^d$ rollouts are finished (but often less in practice), \alg{FSSS} will have expanded the tree as much as is possibly useful, and will agree with the action chosen by \alg{Sparse~Sampling}. Thus, \alg{FSSS} could be viewed as an anytime version of SS that uses pruning to speed its calculation.

The fact that \alg{FSSS} maintains upper bounds on the values for each state it considers will be useful when an optimistic planner is required. It is often the case that accurately finding (or even approximating) a state's value will end up being very difficult or intractable. \alg{FSSS} promises that, with high probability, the true value for a given state will be somewhere in between $U(s)$ and $L(s)$. Maintaining this range of uncertainty can be useful for the algorithm employing \alg{FSSS} as a planning algorithm, as we shall see in Chapter~\ref{chap:bfs3}.

\begin{algorithm}[tb]
	\caption{$\mbox{FSSS}(s, d, C, t, M)$}
	\label{alg:fs3}
	\KwIn{state $s$, max depth $d$, \#trajectories $t$, branching factor $C$, MDP $M$}
	\KwOut{estimated value for state $s$}

	\For {$t$ times} {
		$\mbox{FSSS-Rollout}(s, d, C, 0, M)$
	}
	$\hat V(s) \leftarrow U_d(s)$\\
	\Return $\hat V(s)$
\end{algorithm}

\begin{algorithm}[tb]
	\caption{$\mbox{FSSS-Rollout}(s, d, C, l, M)$}
	\label{alg:fs3-rollout}
	\KwIn{state $s$, max depth $d$, branching factor $C$, current depth $l$, MDP $M$}
	\If{$\mbox{Terminal}(s)$}{
		$U_d(s)=L_d(s)=0$\\
		\Return
	}
	\If {$d=l$} {
		\Return
	}
	\If {$\neg\mbox{Visited}_d(s)$} {
		$\mbox{Visited}_d(s) \leftarrow \mbox{true}$\\
		\ForEach {$a \in A$} {
			$R_d(s,a),\mbox{Count}_d(s,a,s'),\mbox{Children}_d(s,a) \leftarrow 0, 0, \{\}$\\
			\For {$C$ times} {
				$s', r \sim T_M(s, a), R_M(s,a)$ \\
				$\mbox{Count}_d(s,a,s') \leftarrow \mbox{Count}_d(s,a,s') + 1$ \\
				$\mbox{Children}_d(s,a) \leftarrow \mbox{Children}_d(s,a) \cup \{s'\}$ \\
				$R_d(s,a) \leftarrow R_d(s, a)+r/C$\\
				\If {$\neg\mbox{Visited}_{d+1}(s')$} {
					 $U_{d+1}(s'), L_{d+1}(s') \leftarrow \Vmax, \Vmin$
				}
			}
		}
		$\mbox{Bellman-backup}(s, d)$
	}
	$a \leftarrow \argmax_a U_d(s,a)$\\
	$s' \leftarrow \argmax_{s'} (U_{d+1}(s')-L_{d+1}(s')) \cdot \mbox{Count}_d(s,a,s')$\\
	$\mbox{FSSS-Rollout}(s', d, l+1, M)$\\
	$\mbox{Bellman-backup}(s, d)$\\
	\Return
\end{algorithm}

\begin{algorithm}[tb]
	\caption{$\mbox{Bellman-backup}(s, d)$}
	\label{alg:bellman}
	\KwIn{state $s$, depth $d$}
	\ForEach {$a \in A$} {
		$U_d(s,a), L_d(s,a) \leftarrow 0, 0$\\
		\ForEach {$s' \in \mbox{Children}_d(s,a)$} {
			$U_d(s,a) \leftarrow U_d(s,a) + \frac 1 C \gamma U_{d+1}(s')$\\
			$L_d(s,a) \leftarrow L_d(s,a) + \frac 1 C \gamma L_{d+1}(s')$
		}
	}
	$U_d(s) = \argmax_a U_d(s,a)$\\
	$L_d(s) = \argmax_a L_d(s,a)$\\
	\Return
\end{algorithm}

%
\ifperchapterbib%
For the convenience of the reader, a list of references is provided at the end of each chapter (where applicable).
\ifendbib%
%A bibliography containing all cited references is included at the \hyperref[sec:bibliography]{end of the dissertation}.
\else\fi% end ifendbib
%\cbend%
\else\fi% end ifperchapterbib
