\label{chap:bfs3}


\section{Introduction}

In this chapter we discuss Bayesian Forward Search Sparse Sampling, or {\bf BFS3}.

\section{Building blocks}

{\bf BFS3} is built upon the ideas from a few existing algorithms.

\subsection{Sparse Sampling}

{\bf Sparse Sampling}~\cite{kearns99b} works by recursively expanding a full search tree up to a certain depth $d$. At the root, each of the $A$ actions is chosen a constant number of times $C$, yielding a set of $A\cdot C$ children. Sparse sampling is then run on each of the children with a recursion depth one less than the root's. Once the tree is fully created, the leaves are each assigned a value of zero. Then, starting at the leaves, the values are backed up and combined via the Bellman equation, giving the parents' values, until the root's value is determined. The total number of nodes in this search tree is $(AC)^d$, making the algorithm impractical to run in all but the most trivial of domains.

It is worth noting, however, that {\bf Sparse Sampling} is best known as one of the first RL planning algorithms that can achieve high accuracy with high probability using an amount of computation that is not a function of the size of the state space\footnote{Assuming sampling from the model in constant time}. Because of this attractive property, it makes sense to select it or one of its variants as the planner for the infinitely large BAMDP.  {\bf Sparse Sampling} is the basis for a number of Monte-Carlo Tree Search (MCTS) algorithms, which are considerably faster in practice \cite{kocsis06,walsh10,wang05}.

{\bf Spare Sampling} is discussed in more detail in Chapter~\ref{sec:relmbbrl}.

\subsection{Forward Search Sparse Sampling}

{\bf Forward Search Sparse Sampling}, or {\bf FSSS}, is a Monte-carlo tree search algorithm used for planning in MDPs. It is the approach we adopt in this paper.  It preferentially expands the search tree through the use of rollouts, and is outlined in Algorithm~\ref{alg:fs3}. Unlike either {\bf Bayesian Sparse Sampling} or {\bf UCT}, it retains the attractive guarantees of the original {\bf Sparse Sampling} algorithm. {\bf FSSS} maintains hard upper and lower bounds on the values for each state and action so as to direct the rollouts; actions are chosen greedily according to the upper bound on the value, and the next state is chosen such that it is the most uncertain of the available candidates (according to the difference in its upper and lower bounds).

{\bf FSSS} will find the action to take from a given state $s_0$, which will be the root of the search tree.  The tree is expanded by running $t$ trajectories, or rollouts, of length $d$. There are theoretically justified ways to choose $t$ and $d$, but in practical applications they are knobs used to balance computational overhead and accuracy. To run a single rollout, the agent will call Algorithm~\ref{alg:fs3-rollout}, $\mbox{FSSS-Rollout}(s_0, d, 0, M)$.
%, $T$ times. 
The values $U_d(s)$ and $L_d(s)$ are the upper and lower bounds on the value of the node for state $s$ at depth $d$, respectively. Each time a rollout is performed, the tree will be expanded. After at most $(AC)^d$ rollouts are finished (but often less in practice), {\bf FSSS} will have expanded the tree as much as is possibly useful, and will agree with the action chosen by {\bf Sparse Sampling}.

\begin{algorithm}[tb]
	\caption{$\mbox{FSSS}(s, d, t, M)$}
	\label{alg:fs3}
	\KwIn{state $s$, max depth $d$, \#trajectories $t$, MDP $M$}
	\KwOut{estimated value for state $s$}

	\For {$t$ times} {
		$\mbox{FSSS-Rollout}(s, d, 0, M)$
	}
	$\hat V(s) \leftarrow \max_a U_d(s, a)$\\
	\Return $\hat V(s)$
\end{algorithm}

\begin{algorithm}[tb]
	\caption{$\mbox{FSSS-Rollout}(s, d, l, M)$}
	\label{alg:fs3-rollout}
	\KwIn{state $s$, max depth $d$, current depth $l$, MDP $M$}
	\If{$\mbox{Terminal}(s)$}{
		$U_d(s)=L_d(s)=0$\\
		\Return
	}
	\If {$d=l$} {
		\Return
	}
	\If {$\neg\mbox{Visited}_d(s)$} {
		$\mbox{Visited}_d(s) \leftarrow \mbox{true}$\\
		\ForEach {$a \in A$} {
			$R_d(s,a),\mbox{Count}_d(s,a,s'),\mbox{Children}_d(s,a)$\\
			\ \ \ $\leftarrow 0, 0, \{\}$\\
			\For {$C$ times} {
				$s', r \sim T_M(s, a), R_M(s,a)$ \\
				$\mbox{Count}_d(s,a,s') \leftarrow \mbox{Count}_d(s,a,s') + 1$ \\
				$\mbox{Children}_d(s,a) \leftarrow \mbox{Children}_d(s,a) \cup \{s'\}$ \\
				$R_d(s,a) \leftarrow R_d(s, a)+r/C$\\
				\If {$\neg\mbox{Visited}_{d+1}(s')$} {
					 $U_{d+1}(s'), L_{d+1}(s') = \Vmax, \Vmin$
				}
			}
		}
		$\mbox{Bellman-backup}(s, d)$
	}
	$a \leftarrow \argmax_a U_d(s,a)$\\
	$s' \leftarrow \argmax_{s'} (U_{d+1}(s')-L_{d+1}(s')) \cdot \mbox{Count}_d(s,a,s')$\\
	$\mbox{FSSS-Rollout}(s', d, l+1, M)$\\
	$\mbox{Bellman-backup}(s, d)$\\
	\Return
\end{algorithm}

\section{Bayesian Forward Search Sparse Sampling}


\begin{algorithm}[tb]
	\caption{$\mbox{BFS3}(s, h, d, t, \phi)$}
	\label{alg:bfs3}
	\KwIn{state $s$, history $h$, depth $d$, \#trajectories $t$, MPD prior $\phi$}
	\KwOut{action to take in state $s$}
	\If {$\langle s,h\rangle \in \mbox{solved-belief-states}$} {
		\Return $\pi(\langle s,h \rangle)$
	}
	\ForEach {$a \in A$} {
		\For {$C$ times} {
			$\langle s', h'\rangle , r \sim {T\mbox{-}R}_\phi(\langle s,h\rangle, a)$
			$q(a) \leftarrow q(a) + \frac 1 C \left[r+ \gamma FSSS(\langle s', h'\rangle, d, t, M_\phi)\right]$
		}
	}
	$\mbox{solved-belief-states} \leftarrow \mbox{solved-belief-states} \bigcup \{\langle s,h\rangle\}$\\
	$\pi(\langle s,h\rangle) \leftarrow \argmax_a q(a)$\\
	\Return $\pi(\langle s,h\rangle)$\\
\end{algorithm}


{\bf BFS3} is the application of {\bf FSSS} to a Bayes-adaptive MDP, or BAMDP, and is outlined in Algorithm~\ref{alg:bfs3}. The BAMDP is defined by the MDP prior~$\phi(M)$, and the joint transition and reward function ${T\mbox{-}R}_\phi$ is constructed such that
\begin{eqnarray*}
P(\langle s', h\cup (s,a,s',r)\rangle, r | \langle s, h\rangle, a) &=& \int_M P(s', r | s, a, M)\phi(M|h) dM.
\end{eqnarray*}

Here, the BAMDP's state-space is the set of belief-states that include the history of all transitions seen so far. Because of how the BAMDP's transition function is constructed, the set of next-states that an agent can transition to always have a history that includes that transition.

Since, with {\bf FSSS}, the next belief-states are only sampled and their likelihoods are never calculated, a simple generative process can be used:
\begin{eqnarray}
M &\sim& \phi|h \label{eq:oracle1}\\
s', r &\sim& T_M(s,a), R_M(s,a).\label{eq:oracle2}
\end{eqnarray}

This process is used whenever {\bf BFS3} or its subroutine {\bf FSSS} sample a next-state and reward. The algorithm never holds on to an individual MDP after a single transition has been sampled from it. Also, note that whenever {\bf FSSS} does a Bellman backup, that backup is done for a belief-state (since {\bf FSSS} is acting on the BAMDP).

First, an MDP $M$ is sampled from the posterior $\phi|h$. Sometimes this can be a computationally expensive action, since inference is generally a hard probelm. The {\bf BFS3} algorithm can only work effectively if this step is fast. Once it is sampled, then the next state and reward are sampled from $M$.

To reconstruct the resulting belief-state, we pack the resulting concrete state $s'$ with the new history made by augmenting $h$ with $(s, a, s', r)$, resulting in a transition from belief-state $\langle s, h\rangle$ to $\langle s', h\cup (s,a,s',r)\rangle$.

Figure~\ref{fig:bfs3} illustrates the {\bf BFS3}'s process for each belief-state visited by the agent. In the future, the agent may find itself in one of the reachable belief-states in the search tree.
%shows how an agent can step through the search tree while making observations. The grayed trail represents the agent's history. The history for any particular node is the path from the root to that node.

In many cases, the history $h$ can be summarized by a more compact sufficient statistic, and next-state posteriors can be sampled efficiently (for example, the {\bf FDM} prior detailed in Section~\ref{sec:bg}).

The sufficient statistic for {\bf FDM} (and for any discrete state and discrete action MDP) is the set of histograms indicating how many times each state $s'$ has resulted from a particular state-action pair. Due to conjugacy, next-states can be sampled using {\bf FDM} without sampling an intermediate MDP. The next-state distribution is a multinomial with weights proportional to the corresponding histogram added to the prior vector $\alpha$.

\begin{figure}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.5]{bfs3}}
\caption{
%{\bf Left:} the search tree when the agent is at state $s_0$ with no further observations. {\bf Right:} the agent has taken a single step in the world by performing action $a_1$, resulting in a transition to state $s_2$. White nodes indicate hypotheticals that may or may not happen. Grayed nodes indicate things that have been observed. Blackened nodes indicate data that is no longer pertinent and can be thrown out.
{\bf BFS3} samples next-belief-states for each action $C$ times, and then runs {\bf FSSS} on the resulting belief state, using the BAMDP as a generative model. Every node the same distance from the root represents one of the possible worlds that the agent may experience, each with a different history and MDP posterior.
}
\label{fig:bfs3}
\end{center}
\vskip -0.2in
\end{figure} 

\subsubsection{Theorem statement}

If: \begin{inparaenum} \item \label{theorem:cond-fsss} {\bf FSSS}, given the true MDP $m_0$ and some bound on computational resources, can provide accurate value estimates with high probability, and \item \label{theorem:cond-acc} the posterior next-state distribution for some state-action pair, given $N$ examples, will be an accurate estimate of $m_0$'s next-state distribution with high probability\end{inparaenum}, then: with high probability, {\bf BFS3} will behave Bayes-optimally for all but a polynomial number of steps.

\subsection{Proof sketch}


First, we will show that there is a BAMDP, constructed from the prior $\phi$, whose optimal policy is the Bayes-optimal policy for $m_0\sim \phi$. Then, we will show that {\bf BFS3}, acting in the BAMDP, will satisfy the three criteria required for PAC-MDP behavior~\cite{kakade03,lihong09abr2} in that BAMDP\footnote{PAC-MDP behavior in the BAMDP implies near Bayes-optimal behavior in the learning setting.}.
These criteria are: \begin{inparaenum} \item accuracy, \item bounded discoveries, and \item optimism. \end{inparaenum}

First, because of Condition~\ref{theorem:cond-acc} in our theorem statement, we know that once we have received $N$ examples of transitions from a state-action pair $(s,a)$, our estimate of the next-state distribution for that pair will be accurate.  (This condition need not hold for degenerate priors, but it appears to hold quite broadly.)

Second, since we forget all additional transitions from state-action pairs for which we have seen $N$ examples, the number of possible state-histories that an agent can observe is bounded.  Specifically, each time a transition from some state-action $(s,a)$ is observed, either no change will be made to the state-action's histogram (it already sums to $N$), or exactly one entry in the histogram will be incremented by $1$.  Since the histogram can be changed at most $N$ times, the total number of histories possible for an agent over the course of a single experiment is $N \cdot S \cdot A$ ($N$ histories for each state-action pair).

A discovery event, or one that potentially changes the MDP posterior, is an event that results in a change to the history. There are $N \cdot S \cdot A$ discoveries possible, since other transitions will be forgotten.

Third, $\mathbf{FSSS}(s',d,t,M_\phi)$ is guaranteed to have an optimistic value estimate for belief-state $s'$ as $t$ (the number of trajectories), our bounded resource, grows smaller. We also know that, from Condition~\ref{theorem:cond-fsss} of the theorem, $t$ is sufficient to find accurate estimates of $s'$ if all states in $s'$'s subtree have converged next-state posteriors. Simply put, if $s'$'s subtree has no unknown state-action pairs, then {\bf FSSS}'s estimate of that state's value will be accurate.  As a result, if {\bf FSSS}'s estimate of a state's value is inaccurate, there must be something to learn about in $s'$'s subtree. {\bf FSSS} guarantees that this inaccuracy will be optimistic.

Also possible is that the value estimate of $s'$ is accurate \emph{and} there are unknown states in its subtree. In this case, the agent can decide whether or not to visit that state fully informed of its value, and can take a Bayes-optimal action.

The PAC-MDP criteria direct the agent to areas of either high value or high uncertainty, managing the exploration/exploitation tradeoff. Because the agent will only go to areas of high uncertainty over areas of high reward a bounded number of times that grows linearly with the number of possible discovery events, we bound the number of sub-optimal steps taken over the lifetime of the agent.

\section{Proof}


This section presents the detailed argument that {\bf BFS3} is near
Bayes-optimal.

\subsection{Terms}
\begin{itemize}
\item $N$ is the maximum number of transitions that will be observed from any particular state-action pair. All subsequent transitions will not be recorded.
\item The history $h\in H$ is the collection of all observed transitions $(s,a)\rightarrow s'$.
\item $\phi$ is the MDP prior. $\phi|h$ is the MDP posterior.
\item $m_0$ is the unknown true MDP.
\item $P_m(s'|s,a)$ is the probability of going from $s$ to $s'$ when performing action $a$ in some MDP~$m$.
\item $P_{\phi|h}(s'|s,a)=\int_m \phi(m|h)P_m(s'|s,a) dm$ is the posterior transition likelihood, integrating out all possible MDPs.
\item $V_m(s)$ is the true value of state $s$ according to MDP $m$.
\item $\mbox{\bf FSSS}_m(s, B)$ is the value of state $s$ estimated by {\bf FSSS} when using $s$ as a root, $m$ as an oracle (that samples next-states given a state-action pair), and with computational resource bounds represented by $B$. In this context, $B$ is the number of trajectories and the search depth that {\bf FSSS} uses to perform roll-outs.
\end{itemize}

\subsection{Prerequisites}
\begin{enumerate}
\item \label{fsss-acc} The {\bf planning assumption}: For any $s$, w.p.\ $1-\delta_p$, $|\mbox{\bf FSSS}_{m_0}(s, B)-V_{m_0}(s)| \leq \epsilon_p$.\\
In other words, {\bf FSSS} will estimate values accurately if it is given the true model and at least $B$ computation time. 
\item \label{sa-bound} The {\bf accuracy assumption}: For some state-action pair $(s,a)$, if $h$ includes $N$ samples from $P_{m_0}(s'|s,a)$, then w.p.\ $1-\delta_c/(S A)$,
$$\forall_{s'}~|P_{\phi|h}(s'|s,a) - P_{m_0}(s'|s,a)| \leq \alpha_c.$$
As a result, if $N$ examples of each of the $S\cdot A$ state-action pairs are observed, w.p.\ $1-\delta_c$, all transition probability deviations are bounded by $\alpha_c$.

Note: this condition removes the need to say $m_0 \sim \phi$. All of our closeness assumptions are already met.
%\item \label{small-N} $N$ is polynomial in the parameters of the problem.
\item The {\bf MCTS simulation conjecture}, described in Section~\ref{mcts-conj}.
\end{enumerate}



\subsection{MCTS simulation conjecture}
\label{mcts-conj}

Let an MCTS algorithm $\mathcal{A}_m(s,B)$ be an estimator of the value of state $s$ in MDP $m$, bounded by computational resources $B$, whose sole source of stochasticity comes from using $m$ as a generator of $s'\sim s,a|m$. {\bf FSSS} and {\bf UCT}~\cite{kocsis06} both fall into this category; their resources are the number of roll-outs and maximum search depth.

Intuitively, we conjecture that if $\mathcal{A}$ can reliably find accurate values for states in one MDP, then it can also reliably find accurate values for the MDP when using an approximate second MDP as its next-state generator.

If,
\begin{enumerate}
\item \label{mcts-bound} for all $s,a,s'$, $|P_{m_a}(s'|s,a) - P_{m_b}(s'|s,a)| \leq \alpha_c$,
\item \label{mcts-good} for all $s$, w.p.\ $1-\delta_p/(CA)$, $|\mathcal{A}_{m_a}(s,B)-V_{m_a}(s)|\leq \epsilon_p$,
\end{enumerate}
then
\begin{itemize}
\item for all $s$, w.p.\ $1-\delta_{p'}/(CA)$, $|\mathcal{A}_{m_b}(s,B)-V_{m_a}(s)|\leq \epsilon_{p'}$,
\end{itemize}
where $1/\delta_{p'}=\mbox{poly}(1/(1-\alpha_c),1/\delta_p,1/\epsilon_p,m_a,B)$ and $\epsilon_{p'}=\mbox{poly}(1/(1-\alpha_c),1/\delta_p,1/\epsilon_p,m_a,B)$.

\subsubsection{Note on $C \cdot A$}
The probability $1-\delta_{p}/(CA)$ is used because, for each step, {\bf BFS3} will invoke {\bf FSSS} $C \cdot A$ times, or $C$ times for each action.  This choice was made to ensure that all next states get enough resources to ensure accuracy when their dynamics are known. We use the union bound to say, w.p.\ $1-\delta_p$, \emph{all} such next states have accurate estimates.

\subsection{PAC-MDP behavior in the BAMDP}

Let
\begin{itemize}
\item the BAMDP be $m_{\phi|h}$,
\item the set of all possible histories be $H$,
\item the state space of the BAMDP be the set of belief-states $\mathbb{S} = S \times H$, and
\item the action space of the BAMDP be the same as the regular MDP.
\end{itemize}

We shall limit $H$ to contain only histories with at most $N$ examples of transitions from any state-action pair. That is, when the agent observes the $N_{\mbox{th}}$ transition from state $s$ and action $a$, it shall never again update its history when making a transition from that state with that action. This forgetfulness causes the set $H$ to be finite.

In a discrete state and action MDP, the history $h\in H$ may be summarized by a set of histograms, one for each state-action pair. Since we ignore all transitions from any state-action pair after the $N_{\mbox{th}}$ transition from that state-action pair, we know that the sum of all entries in all histograms cannot exceed $S\cdot A \cdot N$.

Since, whenever the history \emph{is} updated, a single entry for a single histogram will be incremented by $1$, we can infer that at most $S\cdot A \cdot N$ unique histories can possibly be experienced by an agent over the course of a single experiment.

The agent can also visit at most $S$ true states (all of them). Since there are $S$ true states and $S\cdot A \cdot N$ possible histories, the number of belief-states that an agent can visit, in a single experiment, is $M = S^2\cdot A \cdot N$.

Our strategy will be to say that, for each of these at most $M$ possible belief-states visited by the agent, the agent will either choose an action that is approximately optimal in the BAMDP (and therefore approximately Bayes-optimal in the true MDP), or exploratory in the BAMDP (with no guarantees about how good this action is in the true MDP).

We say that an agent is near Bayes-optimal if we can limit the number of exploratory actions taken to a polynomial of the domain parameters.

To show PAC-MDP behavior in the BAMDP (and, therefore, near Bayes-optimal behavior), we need to show that three conditions hold~\cite{lihong09abr2}:
\begin{enumerate}
\item \label{pacmdp-disc} bounded discoveries,
\item \label{pacmdp-acc} accuracy, and
\item \label{pacmdp-opt} optimism.
\end{enumerate}

To understand these criteria and how they connect PAC-MDP behavior in the BAMDP to near Bayes-optimal behavior in the true MDP, we need to define the concept of \emph{known} and \emph{unknown} \emph{states} in $m_0$ and  \emph{known} and \emph{unknown} \emph{belief-states} in $m_\phi$.

A \emph{state} $s$ is considered known if the history $h$ contains $N$ examples of transitions from $s$ for each action $a\in A$.

A \emph{belief-state} $\langle s, h \rangle$ is considered known if {\bf FSSS}'s estimate of its value is accurate.

Figure~\ref{fig:bfs3tree} illustrates how unknown states in a belief-state's {\bf FSSS} subtree may cause a belief-state's estimate to be inaccurate. Because of Prerequisite~\ref{fsss-acc}, the planning assumption, we know that if there are no unknown states in a belief-state's subtree, then the belief-state's estimate will be accurate, and we can consider the belief-state to be known.

It is possible for a belief-state to be known if there are unknown states in its subtree; in the limit of computation, {\bf FSSS} will be accurate for all belief-states. But, it is not possible for all states in the belief-state's subtree to be known if the belief-state is not known.

As a result, an unknown belief-state indicates an unknown state that is at most $D$ steps away, where $D$ is the search depth given to {\bf FSSS}. Since unknown belief-states always have optimistic estimates when they are evaluated with {\bf FSSS}, the agent is pulled towards unknown states.

\begin{figure}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.5]{bfs3tree}}
\caption{
{\bf BFS3} will run {\bf FSSS} on each of the $C\cdot A$ next belief-states, $\langle s', h' \rangle$ and its peers. Triangles below belief-states represent search trees explored by {\bf FSSS}. A greyed search tree indicates that some of the true states visited in that search tree are unknown, and the value estimate may be inaccurate. Because we are using {\bf FSSS}, an inaccurate estimate is guaranteed to be optimistic, and can bubble up to the root.
}
\label{fig:bfs3tree}
\end{center}
\vskip -0.2in
\end{figure} 

\subsubsection{Satisfying the PAC-MDP criteria}

{\bf BFS3} is PAC-MDP for the belief-state MDP because it satisfies the 3 criteria.

Condition~\ref{pacmdp-disc} holds because if the agent reaches an unknown belief state (one with an inaccurate, optimistic value), it must be because there is an unknown state-action pair beneath it in the tree \emph{and} this node will be reached by the current policy with high probability.  (Quantifying this claim involves an argument that closely parallels the ``explore or exploit'' lemma from PAC-MDP theory.)  Note that when an unknown state-action pair is reached, it moves closer to being known.  In fact, an agent can only do this update $S\cdot A \cdot N$ times, so, the number of discoveries is bounded.

% Condition~\ref{pacmdp-disc} holds because a discovery is an event that updates the posterior, or changes the history. Since an agent can only do this update $S\cdot A \cdot N$ times, the number of discoveries is bounded by $S\cdot A \cdot N$.

Condition~\ref{pacmdp-acc} holds because of Prerequisite~\ref{sa-bound} (the accuracy assumption). Once we have $N$ examples of a state-action pair, we have an accurate estimate of the next-state distribution for that pair. Once we have $N$ examples of all state-action pairs, we have an accurate estimate of the entire MDP.

%Condition~\ref{pacmdp-opt} holds because of the nature of {\bf FSSS}. We know that {\bf FSSS} will never be pessimistic about a state's value~\cite{walsh10}. We will allow any uncertainty in a belief-state's value (that is, it can possibly run into a state-action pair for which $N$ samples have \emph{not} been observed) to cause {\bf FSSS} to be inaccurate. And, if it is not accurate, {\bf FSSS} must be optimistic. In this sense, uncertainty will draw the agent towards unknown states.

Condition~\ref{pacmdp-opt} holds because of our definition of known and unknown belief-states and the behavior of {\bf FSSS}. Known belief-states have accurate values, and unknown belief-states have optimistic values. No belief-state ever has a pessimistic value.

\subsection{How accurate, and how likely?}

We have shown that {\bf BFS3} is near Bayes-optimal, meaning with high probability, it will be accurate for all but a small number of steps. This section details how accurate {\bf BFS3} is and with what probability it achieves this accuracy.

\subsubsection{Bayes-optimal behavior for a converged posterior}
\label{sec-conv}
We declare the posterior to be converged if, for each $s,a$, our history $h$ includes $N$ examples of a transition from $s,a$.

Let $m_0$ and $\phi|h$ take the place of $m_a$ and $m_b$ in the MCTS simulation conjecture.
By Prerequisite~\ref{sa-bound}, we have Condition~\ref{mcts-bound} for the MCTS simulation conjecture. 
By Prerequisite~\ref{fsss-acc}, we have Condition~\ref{mcts-good} for the MCTS simulation conjecture.
For each step in the environment, {\bf BFS3} runs {\bf FSSS} $C$ times for each action, resulting in $C\cdot A$ executions of {\bf FSSS}. We can put a lower bound on the likelihood that they are all $\epsilon_{p'}$-accurate of $1-\delta_{p'}$.

The estimate for the value of taking action $a$ from the root state $s_0$ is calculated as
\begin{equation}
\hat Q(s_0, a) = R(s_0,a) + \gamma \frac 1 C \sum_{i=1}^C \mbox{\bf FSSS}_{\phi|h}(s_i, B).
\end{equation}

Let the Monte~Carlo estimate of $Q(s_0,a)$, using the true value for all next states, be
\begin{equation}
\tilde Q(s_0, a) = R(s_0,a) + \gamma \frac 1 C \sum_{i=1}^C V^*(s_i).
\end{equation}

From the Bellman equation, we know that
\begin{equation}
Q^*(s_0, a) = R(s_0,a) + \gamma \sum_{s_i} P_{m_0}(s_i|s,a) V^*(s_i).
\end{equation}

First, let's bound the difference between $\hat Q(s_0, a)$ and $\tilde Q(s_0, a)$.
\begin{eqnarray}
\nonumber |\hat Q(s_0, a) - \tilde Q(s_0, a)| &=& \left|\gamma \frac 1 C \sum_{i=1}^C \mbox{FSSS}_{\phi|h}(s_i, B) - \gamma \frac 1 C \sum_{i=1}^C V^*(s_i)\right|\\
\nonumber &=& \left|\gamma \frac 1 C \sum_{i=1}^C \left(\mbox{FSSS}_{\phi|h}(s_i, B) -  V^*(s_i)\right)\right|\\
\nonumber &\leq& \gamma \frac 1 C \sum_{i=1}^C \left|\mbox{FSSS}_{\phi|h}(s_i, B) -  V^*(s_i)\right|\\
\nonumber &\leq& \gamma \frac 1 C \sum_{i=1}^C \epsilon_{p'}\\
|\hat Q(s_0, a) - \tilde Q(s_0, a)| &\leq& \epsilon_{p'}.
\end{eqnarray}

Second, let's bound the difference between $\tilde Q(s_0, a)$ and $Q^*(s_0, a)$.
\begin{eqnarray}
\nonumber |Q^*(s_0, a) - \tilde Q(s_0, a)| &=& \left|\gamma \sum_{s_i} P_{m_0}(s_i|s,a)V^*(s_i) - \gamma \frac 1 C \sum_{i=1}^C V^*(s_i)\right|\\
\nonumber &=& \gamma \left|\sum_{s_i} P_{m_0}(s_i|s,a)V^*(s_i) - \frac 1 C \sum_{i=1}^C V^*(s_i)\right|\\
|Q^*(s_0, a) - \tilde Q(s_0, a)| &\leq& \lambda,
\end{eqnarray}
w.p.\ $1-e^{-\lambda^2 C / {\Vmax^2}}$~\cite{kearns99b}.

This bounds the error in the estimated Q-values:
\begin{eqnarray}
\nonumber |\hat Q(s_0, a) - Q^*(s_0, a)| &=& |\hat Q(s_0, a) - \tilde Q(s_0, a) + \tilde Q(s_0, a) - Q^*(s_0, a)|\\
\nonumber &\leq& |\hat Q(s_0, a) - \tilde Q(s_0, a)| + |\tilde Q(s_0, a) - Q^*(s_0, a)|\\
|\hat Q(s_0, a) - Q^*(s_0, a)| &\leq& \epsilon_{p'}+\lambda,
\end{eqnarray}

w.p.\ $1 - \delta_{p'} - e^{-\lambda^2 C / {\Vmax^2}}$.

Let $\epsilon = \epsilon_{p'}+\lambda$, and $\delta = \delta_c + S\left(\delta_{p'} + e^{-\lambda^2 C / {\Vmax^2}}\right)$.

Once the model has converged, every time the agent takes a step from some state $s_0$ it remembers what action it took, and will take that action in the future. The likelihood of it choosing an $\epsilon$-optimal action for each of the $S$ possible states is no less than $1-\delta$.

\subsubsection{Bayes-optimal behavior for an unconverged posterior}

Since the agent will not update the history $h$ on a transition from $s,a$ when transitions $s,a$ have been observed $N$ times in the past, we know there are only $S \cdot A \cdot N$ different histories possible over the course of a single experiment.

Given a particular history $h$ and state $s$ that has been experienced before, {\bf BFS3} will choose the same action as last time, limiting the total amount of computation possible, and also the total number of times {\bf BFS3} has to succeed is limited at $M = S^2 \cdot A \cdot N$, or the number of states times the number of possible histories.

In Sections~\ref{sec:inf}~and~\ref{sec:fin}, we set conditions for {\bf BFS3} to choose either an exploratory action or an optimal action for each of these $M$ possible events.

On a given step, {\bf BFS3} will run {\bf FSSS} on each of $A$ possible actions and, for each action, $C$ possible next-states.


\subsubsection{In the limit of infinite computation}
\label{sec:inf}

Each of the next-state possibilities will be fully explored and FSSS will agree with Sparse Sampling, giving an error no more than 
$$\epsilon_\infty = \frac {\lambda(1-\gamma^{D+1})} {1-\gamma}+\gamma^D\Vmax,$$
w.p.\ at least $1-(C\cdot A)^D e^{-\lambda^2C/\Vmax^2}$, where $D$ is the maximum search depth. Let $\delta_\infty/M=(C\cdot~A)^De^{-\lambda^2C/\Vmax^2}$; the agent will then be able to pick an $\epsilon_\infty$-optimal action for each of the $M$ events w.p.\ at least $1-\delta_\infty$.

\subsubsection{Limited computation}
\label{sec:fin}

In cases where there is limited computation and all states in the subtree have converged according to Prerequisite~\ref{sa-bound}, we are guaranteed to act $\epsilon_f$-optimal w.p.\ at least $1 - \delta_f/M$, according to Section~\ref{sec-conv}, where
$$\epsilon_f = \epsilon_{p'}+\lambda$$
and
$$\delta_f/M = \delta_{p'} + e^{-\lambda^2 C / {\Vmax^2}}.$$

In cases with limited computation and when there are states in the subtree that are not converged, we are guaranteed to have an $\epsilon_f$-\emph{optimistic} estimate w.p.\ at least $1-\delta_f/M$. This is true because of the guarantees provided by {\bf FSSS}.

An action $a$'s estimate is $\epsilon$-optimistic if
$$\hat Q(s, a) \geq Q^*(s, a) - \epsilon.$$

Note that an $\epsilon$-optimal estimate is also $\epsilon$-optimistic.

Since for each of the $M$ events we have an $\epsilon_f$-optimistic estimate w.p.\ $1-\delta_f/M$, we will have an $\epsilon_f$-optimistic estimate for each event simultaneously w.p.\ $1-\delta_f$.

%We can now lean on the PAC-MDP proof for $\epsilon_f$-optimal behavior in the belief MDP w.p.\ $1-\delta_f$ for all but a polynomial number of steps.

%If we take an action whose estimate is $\epsilon_f$-optimistic but not $\epsilon_f$-optimal, that means that there must be something worth learning (in that if we knew it, it has the potential to change our estimate) in the next $D$ steps.

%
\ifperchapterbib%
For the convenience of the reader, a list of references is provided at the end of each chapter (where applicable).
\ifendbib%
%A bibliography containing all cited references is included at the \hyperref[sec:bibliography]{end of the dissertation}.
\else\fi% end ifendbib
%\cbend%
\else\fi% end ifperchapterbib
